models:
- name: Llama 3.1 Instruct
  displayName: Llama 3.1 Instruct
  modelHubID: llama-3.1-instruct
  category: Text Generation
  type: NGC
  description: The Llama 3.1 70B-Instruct, 8B instruct and 8B base NIM simplifies the deployment of the Llama 3.1 70B-Instruct, 8B instruct and 8B base tuned models which is optimized for language understanding, reasoning, and text generation use cases, and outperforms many of the available open source chat models on common industry benchmarks.
  requireLicense: true
  licenseAgreements:
  - label: Use Policy
    url: https://llama.meta.com/llama3/use-policy/
  - label: License Agreement
    url: https://llama.meta.com/llama3/license/
  modelVariants:
  - variantId: Llama 3.1 8B Instruct
    modelCard: {
    "accessType": "NOT_LISTED",
    "application": "Other",
    "bias": "",
    "canGuestDownload": false,
    "createdDate": "2025-02-03T23:30:51.257Z",
    "description": "# **Llama-3.1-8B-Instruct Overview**\n\n## **Description:**\n\n**Llama-3.1-8B-Instruct** is an 8 billion parameter, instruction-tuned large language model created by Meta. This model is part of the Llama 3.1 family of open-access models and is specifically optimized for dialogue and conversational use cases, making it highly capable of following user instructions to perform a wide variety of natural language processing tasks.\n\nThis model is ready for commercial/non-commercial use.\n\nThis version introduces support for GB200 NVL72, GH200 NVL2, B200 and NVFP4. CUDA updated to version 12.9. For detailed information, refer to Release [Notes for NVIDIA NIM for LLMs LLM 1.12](https://docs.nvidia.com/nim/large-language-models/latest/release-notes.html). \n\n## **Third-Party Community Consideration**\n\nThis model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIA \\[meta-llama/Llama-3.1-8B-Instruct\\]  \n([https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)). \n\n## **License/Terms of Use:**\n\n**GOVERNING TERMS:** The NIM container is governed by the [NVIDIA Software License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) and the [Product-Specific Terms for NVIDIA AI Products](https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/); and the use of this model is governed by the [NVIDIA Community Model License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/). \n\n**ADDITIONAL INFORMATION**: [Llama 3.1 Community License Agreement](https://www.llama.com/llama3_1/license/). Built with Llama.\n\nYou are responsible for ensuring that your use of NVIDIA provided models complies with all applicable laws.\n\n## **Deployment Geography:**\n\nGlobal \n\n## **Use Case:**\n\nThis model is primarily used by developers, researchers, and businesses to build and experiment with a wide range of generative AI applications. Its combination of strong performance, efficiency, and an open-access license makes it highly versatile.\n\n* Developers and Businesses would use this model to create production-ready applications such as:  \n  * AI Chatbots and customer service agents.  \n  * Content creation tools for writing emails, marketing copy, and articles.  \n  * Summarization and question-answering systems for internal documents.  \n  * Code generation assistants to help programmers write and debug code.  \n* Researchers would use it to study large language model behavior, explore AI safety and alignment, and benchmark new training or fine-tuning techniques on a capable, open model.  \n* AI Hobbyists would use the model for personal projects, running it on consumer-grade hardware to experiment with creating their own AI assistants or exploring the frontiers of generative AI.\n\n## **Release Date:**\n\nBuild.Nvida.com 07/23/2024 via  \n(https://build.nvidia.com/meta/llama-3_1-8b-instruct)\n\nHuggingface 07/23/2024 via   \n(https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)\n\n## **Reference(s):** \n\n[https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) \n\n## **Model Architecture:** \n\nArchitecture Type: Transformer  \nNetwork Architecture: Llama-3.1-8B\n\nThis model was developed based on Meta Llama-3.1-8B  \n[https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) \n\nNumber of model parameters: 8.03*10^9\n## **Input:**\n\nInput Type(s): Text \n\nInput Format(s): String \n\nInput Parameters: One-Dimensional (1D)\n\nOther Properties Related to Input: The primary input limit is the model's maximum context length, which is 128,000 tokens. All input text must be pre-processed using the model's specific tokenizer to convert the string into a sequence of token IDs. For conversational use, inputs should be formatted using the model's designated chat template.\n\n## **Output:**\n\nOutput Type(s): Text \n\nOutput Format(s): String\n\nOutput Parameters: One-Dimensional (1D)\n\nOther Properties Related to Output: The output consists of a variable-length sequence of tokens from the model's vocabulary. The total length of the input and output cannot exceed the 128,000-token context window. Post-processing is required to detokenize the raw token ID sequence into a human-readable string.\n\n## **Software Integration:**\n\nRuntime Engine: vLLM, TensorRT\n\nSupported Hardware Microarchitecture Compatibility:\n\nNVIDIA Ampere  \nNVIDIA Blackwell  \nNVIDIA Hopper  \nNVIDIA Lovelace \n\nPreferred Operating System(s):\n\nLinux   \nWindows  \nmacOS\n\nThe integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment.\n\n## **Model Version(s):**\n\n*Llama-3.1-8B-Instruct-1.10.1 \n*Llama-3.1-8B-Instruct-1.12.0\n*Llama-3.1-8B-Instruct-1.13.1\n\n## **Usage**\n\n### **Use with transformers**\n\nStarting with transformers \\>= 4.43.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\n\nMake sure to update your transformers installation via pip install \\--upgrade transformers.\n\n```\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\nNote: You can also find detailed recipes on how to use the model locally, with torch.compile(), assisted generations, quantised and more at [huggingface-llama-recipes](https://github.com/huggingface/huggingface-llama-recipes)\n\n### **Tool use with transformers**\n\nLLaMA-3.1 supports multiple tool use formats. You can see a full guide to prompt formatting [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/).\n\nTool use is also supported through [chat templates](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling) in Transformers. Here is a quick example showing a single simple tool:\n\n```\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n    \"\"\"\n    Get the current temperature at a location.\n    \n    Args:\n        location: The location to get the temperature for, in the format \"City, Country\"\n    Returns:\n        The current temperature at the specified location in the specified units, as a float.\n    \"\"\"\n    return 22.  # A real function should probably actually get the temperature!\n\n# Next, create a chat and apply the chat template\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\n```\n\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\n\n```\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n```\n\nand then call the tool and append the result, with the tool role, like so:\n\n```\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n```\n\nAfter that, you can generate() again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling \\- for more information, see the [LLaMA prompt format docs](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/) and the Transformers [tool use documentation](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling).\n\n## **Training, Testing, and Evaluation Datasets:**\n\n### **Training Dataset**\n\n**Data Modality:** Text \n\n**Link:** Undisclosed\n\n**Data Collection Method:** Hybrid: Human, Synthetic, Automated\n\n**Labeling Method:** Hybrid: Human, Automated\n\n**Properties:** \n\nThe model was pre-trained on a dataset of over 15 trillion tokens. This dataset is a high-quality mix of publicly available data, heavily filtered for safety and quality. The instruction fine-tuning dataset is smaller and consists of high-quality prompts, responses, and preference rankings curated by humans.\n\n### **Testing Dataset**\n\n**Link:** Undisclosed\n\n**Data Collection Method:** Hybrid: Human, Automated\n\n**Labeling Method:** Human\n\n**Properties:** \n\nThe testing datasets comprise thousands of individual problems designed to measure model capabilities in specific areas:\n\n* **General Knowledge & Reasoning:** MMLU, DROP, AGIEval, BIG-Bench Hard  \n* **Mathematics:** GSM8K, MATH  \n* **Coding:** HumanEval\n\n### **Evaluation Dataset**\n\n**Link:** Undisclosed\n\n**Data Collection Method:** Hybrid: Human, Automated\n\n**Labeling Method:** Human\n\n**Properties:** \n\nThe evaluation datasets consist of thousands of questions and problems designed to test the model's capabilities in areas like general knowledge, reasoning, mathematics, and coding.\n\n**Base pretrained models**\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3 8B | Llama 3.1 8B | Llama 3 70B | Llama 3.1 70B | Llama 3.1 405B |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| General | MMLU | 5 | macro\\_avg/acc\\_char | 66.7 | 66.7 | 79.5 | 79.3 | 85.2 |\n|  | MMLU-Pro (CoT) | 5 | macro\\_avg/acc\\_char | 36.2 | 37.1 | 55.0 | 53.8 | 61.6 |\n|  | AGIEval English | 3-5 | average/acc\\_char | 47.1 | 47.8 | 63.0 | 64.6 | 71.6 |\n|  | CommonSenseQA | 7 | acc\\_char | 72.6 | 75.0 | 83.8 | 84.1 | 85.8 |\n|  | Winogrande | 5 | acc\\_char | \\- | 60.5 | \\- | 83.3 | 86.7 |\n|  | BIG-Bench Hard (CoT) | 3 | average/em | 61.1 | 64.2 | 81.3 | 81.6 | 85.9 |\n|  | ARC-Challenge | 25 | acc\\_char | 79.4 | 79.7 | 93.1 | 92.9 | 96.1 |\n| Knowledge reasoning | TriviaQA-Wiki | 5 | em | 78.5 | 77.6 | 89.7 | 89.8 | 91.8 |\n| Reading comprehension | SQuAD | 1 | em | 76.4 | 77.0 | 85.6 | 81.8 | 89.3 |\n|  | QuAC (F1) | 1 | f1 | 44.4 | 44.9 | 51.1 | 51.1 | 53.6 |\n|  | BoolQ | 0 | acc\\_char | 75.7 | 75.0 | 79.0 | 79.4 | 80.0 |\n|  | DROP (F1) | 3 | f1 | 58.4 | 59.5 | 79.7 | 79.6 | 84.8 |\n\n**Instruction tuned models**\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3 8B Instruct | Llama 3.1 8B Instruct | Llama 3 70B Instruct | Llama 3.1 70B Instruct | Llama 3.1 405B Instruct |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| General | MMLU | 5 | macro\\_avg/acc | 68.5 | 69.4 | 82.0 | 83.6 | 87.3 |\n|  | MMLU (CoT) | 0 | macro\\_avg/acc | 65.3 | 73.0 | 80.9 | 86.0 | 88.6 |\n|  | MMLU-Pro (CoT) | 5 | micro\\_avg/acc\\_char | 45.5 | 48.3 | 63.4 | 66.4 | 73.3 |\n|  | IFEval |  |  | 76.8 | 80.4 | 82.9 | 87.5 | 88.6 |\n| Reasoning | ARC-C | 0 | acc | 82.4 | 83.4 | 94.4 | 94.8 | 96.9 |\n|  | GPQA | 0 | em | 34.6 | 30.4 | 39.5 | 46.7 | 50.7 |\n| Code | HumanEval | 0 | pass@1 | 60.4 | 72.6 | 81.7 | 80.5 | 89.0 |\n|  | MBPP \\++ base version | 0 | pass@1 | 70.6 | 72.8 | 82.5 | 86.0 | 88.6 |\n|  | Multipl-E HumanEval | 0 | pass@1 | \\- | 50.8 | \\- | 65.5 | 75.2 |\n|  | Multipl-E MBPP | 0 | pass@1 | \\- | 52.4 | \\- | 62.0 | 65.7 |\n| Math | GSM-8K (CoT) | 8 | em\\_maj1@1 | 80.6 | 84.5 | 93.0 | 95.1 | 96.8 |\n|  | MATH (CoT) | 0 | final\\_em | 29.1 | 51.9 | 51.0 | 68.0 | 73.8 |\n| Tool Use | API-Bank | 0 | acc | 48.3 | 82.6 | 85.1 | 90.0 | 92.0 |\n|  | BFCL | 0 | acc | 60.3 | 76.1 | 83.0 | 84.8 | 88.5 |\n|  | Gorilla Benchmark API Bench | 0 | acc | 1.7 | 8.2 | 14.7 | 29.7 | 35.3 |\n|  | Nexus (0-shot) | 0 | macro\\_avg/acc | 18.1 | 38.5 | 47.8 | 56.7 | 58.7 |\n| Multilingual | Multilingual MGSM (CoT) | 0 | em | \\- | 68.9 | \\- | 86.9 | 91.6 |\n\n**Multilingual benchmarks**\n\n| Category | Benchmark | Language | Llama 3.1 8B | Llama 3.1 70B | Llama 3.1 405B |\n| :---- | :---- | :---- | :---- | :---- | :---- |\n| General | MMLU (5-shot, macro\\_avg/acc) | Portuguese | 62.12 | 80.13 | 84.95 |\n|  |  | Spanish | 62.45 | 80.05 | 85.08 |\n|  |  | Italian | 61.63 | 80.4 | 85.04 |\n|  |  | German | 60.59 | 79.27 | 84.36 |\n|  |  | French | 62.34 | 79.82 | 84.66 |\n|  |  | Hindi | 50.88 | 74.52 | 80.31 |\n|  |  | Thai | 50.32 | 72.95 | 78.21 |\n\n## **Technical Limitations** \n\n Testing conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, the model's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying this model in any applications, developers should perform safety testing and tuning tailored to their specific applications. Please refer to available resources including the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development. \n\n## **Inference:**\n\n**Acceleration Engine:** vLLM, TensorRT \n\n**Test Hardware:** \n\n* B200 SXM  \n* H200 SXM  \n* H100 SXM  \n* A100 SXM 80GB  \n* A100 SXM 40GB  \n* L40S PCIe  \n* A10G  \n* H100 NVL  \n* H200 NVL  \n* GH200 96GB\n* GB200 NVL72\n* GH200 NVL2\n* RTX 5090  \n* RTX 4090  \n* RTX 6000 Ada\n\n## **Get Help**\n\n### Enterprise Support\nGet access to knowledge base articles and support cases or [submit a ticket](https://www.nvidia.com/en-us/data-center/products/ai-enterprise-suite/support/).\n\n## **Ethical Considerations:**\n\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\n**You are responsible for ensuring that your use of NVIDIA provided models complies with all applicable laws.**",
    "displayName": "Llama-3.1-8b-instruct",
    "explainability": "",
    "framework": "Other",
    "hasPlayground": false,
    "hasSignedVersion": true,
    "isPlaygroundEnabled": false,
    "isPublic": false,
    "isReadOnly": true,
    "labels": [
        "Llama3.1",
        "NIM",
        "NSPECT-0DQP-LNLV",
        "llama-3.1-8b-instruct",
        "nvaie:model:nvaie_supported",
        "nvidia_nim:model:nimmcro_nvidia_nim",
        "productNames:nim-dev",
        "productNames:nv-ai-enterprise"
    ],
    "latestVersionIdStr": "rtx6000-blackwell-svx2-latency-bf16-qyait9sohq",
    "latestVersionSizeInBytes": 17635976938,
    "logo": "https://assets.ngc.nvidia.com/products/api-catalog/images/llama-3_1-8b-instruct.jpg",
    "modelFormat": "N/A",
    "name": "llama-3.1-8b-instruct",
    "orgName": "nim",
    "precision": "OTHER",
    "privacy": "",
    "productNames": [
        "nim-dev",
        "nv-ai-enterprise"
    ],
    "publicDatasetUsed": {},
    "publisher": "Meta",
    "safetyAndSecurity": "",
    "shortDescription": "The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out).",
    "teamName": "meta",
    "updatedDate": "2025-10-21T17:47:59.613Z"
}
    source:
      URL: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/llama-3_1-8b-instruct-nemo
    optimizationProfiles:
    - profileId: nim/meta/llama-3.1-8b-instruct:a10gx4-latency-bf16-r3bmpcovtw
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct A10Gx4 BF16 Latency
      ngcMetadata:
        09fec372bdcfaee0662140bc5ed522900bb0b0da7cc37ceba6209731dc55a689:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: A10G
            gpu_device: 2237:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: c0232176c2e5374758e3d88ea13e70aa0edca0862c923428f54b85da208960a9
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A10G
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2237:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 19GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H100_NVLx2 FP8 Latency
      ngcMetadata:
        0c87e2871cd7a6ea205a137109c3afde0134ba22c6fe8e978a752287cf561643:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H100_NVL
            gpu_device: 2321:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: cb6ac7eedef673edc08e85f4f3e7525c31f499e5c5f376cbffc05cb8eefe197a
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100_NVL
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2321:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:gb200x1-throughput-fp8-8imkyjutxw
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct GB200x1 FP8 Throughput
      ngcMetadata:
        0cf8ac8bfbf183d8a891e9023d6aa7a1d93f6720e5bd78e578711e3d5b822c52:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 2feaef51b8c016e5c678f39202dfe542c11eb5fc2443749e6c2330f3474aaffe
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GB200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 9GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:h100x2-latency-fp8-cvpqroehhq
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H100x2 FP8 Latency
      ngcMetadata:
        0e0a9fb28e4df4f8a2dcaafbcb03ce1e0b9d27a4e00ec273f27bcc47e7572225:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 4217e8fa6ba7ac9609ee76470bec904253dadbe7fc33a52f715e08791073c501
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2330:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 9GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:gb200x2-latency-fp8-i4razlnzqw
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct GB200x2 FP8 Latency
      ngcMetadata:
        192d34f8204aa5c44b08406f8d98c86c606363ff8a2ca5f608b87a2516313b55:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 0222120a0b05a944b22ed6b0d7376bbe89abef1c05fa6ecd7967199500398864
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GB200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 9GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H200_NVLx1 BF16 Throughput
      ngcMetadata:
        1d7b8b2d964254990181ba7a6e93687275c3372b689d66b6494ad5f788a108a6:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H200_NVL
            gpu_device: 233b:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 2660946198ebbb837e487b333ef86b2ac4cbc37b907151de45f291596625f919
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H200_NVL
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 233B:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct RTX6000_ADAx1 INT4_AWQ Throughput
      ngcMetadata:
        245a4f27515a6291ac239b37f209847384dbadaa5ad155c45d17bcc524594371:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: RTX6000_ADA
            gpu_device: 26b1:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 4d714aa3567eb6e2d72aa08be91bb5fc632e7bbaa645c265104ea1d65eb28efa
            number_of_gpus: '1'
            pp: '1'
            precision: int4_awq
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: INT4_AWQ
      - key: GPU
        value: RTX6000_ADA
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 26B1:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:b200x1-throughput-bf16-zsf8rdhqtw
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct B200x1 BF16 Throughput
      ngcMetadata:
        2465a2b2fc773ea207e312352258ca9a54650fc9ec9740ae96646528556a0916:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 61348d392451059a37d2218d940f4aaf266562d0d6fa156e211f266022d5d26e
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: B200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 16GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct GH200_480GBx1 BF16 Throughput
      ngcMetadata:
        28e1523b3569391509a8e976f17c0b04e21faee7095225076a99636cbb1da858:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: GH200_480GB
            gpu_device: 2342:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 828408fdd397e49bb4256a997d3f85d90c3d9a3e756531b8895cd78a83574aa6
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: GH200_480GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2342:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct A100_SXM4_40GBx1 BF16 Throughput
      ngcMetadata:
        40d4f2dcb13710bf7fcf1d9d41dfeb1b0ff22ba2d266bc2997a81a000fa5d031:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: A100_SXM4_40GB
            gpu_device: 20b0:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 72732266ad2f3d3b824f413f11716f81b87ccb602c3cdda972c7341c0d1e60b5
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A100_SXM4_40GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 20B0:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:h100x1-throughput-fp8-rmqqnk9ima
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H100x1 FP8 Throughput
      ngcMetadata:
        4411bf23579e41275d6a994cd768d9dc2ebbd523253e2844115f24644a5e86b1:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 3950ee02bc0277147b77079c0cc5bc954b9189f7866fbbebc37ece4ec31283f6
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2330:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 9GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:b200x2-latency-bf16-px49bz6jka
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct B200x2 BF16 Latency
      ngcMetadata:
        4a7b681f1dc1dcbc0b98f4c4eaa6bdac6557af058dd878039624c68683e2dee3:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 467e29e38751b085aa13fdc92f6eaf1a08a8c360ef19718a92f64bd507221fd8
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: B200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 17GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:gb200x2-latency-bf16--fupfm1fjg
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct GB200x2 BF16 Latency
      ngcMetadata:
        4b344f09436a75385ad7c78aa224f685d1f92980ccf7ea52f29a52c1ca646b70:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 0a3a8da158191386b506caa79c0bd9787f45009a7e52113b82fcdde0511001d7
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: GB200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 17GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:l40sx2-latency-bf16-aauqggrlkw
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct L40Sx2 BF16 Latency
      ngcMetadata:
        55df9113a4cd134e4ddaeeae43cd33089be30b74380a9bc29d677ed9784a3492:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: L40S
            gpu_device: 26b9:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 9b1feaf6e923581317ff4291ca09856eb403efd7acdeee1c8e787d988ced56ce
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: L40S
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 26B9:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 17GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:h200x2-latency-bf16-zwyr4clzla
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H200x2 BF16 Latency
      ngcMetadata:
        588fa4150abaa001f1357112de2ca65c85c1c86322b3f7d0ca9f1451f40baee5:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H200
            gpu_device: 2335:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 7b377dc3e02bbcef7ec1c0ccee4afc1d99d2409dc7ab6576f1f386ebbedeabc6
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2335:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 17GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:b200x2-latency-nvfp4-urjebtmqkg
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct B200x2 NVFP4 Latency
      ngcMetadata:
        5eaaf502f6dab9ce29e7d034182bb56eeeb3e349633f4561018f27b3069189b3:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 2693061ea8698de078f95517349178ce8894a51a97db468183032cba22ab04ae
            number_of_gpus: '2'
            pp: '1'
            precision: nvfp4
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: B200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 6GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H100_NVLx1 FP8 Throughput
      ngcMetadata:
        785d7d60df3f153a36413f29a16ac14bc5cfba73004bc7feee2bca9d78b10e6f:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H100_NVL
            gpu_device: 2321:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: a0de60706fabf3ee071fef41f0c14225a3d88799bb9728af810e57a7499f038f
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100_NVL
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2321:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:b200x1-throughput-nvfp4-6zvdbhtdna
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct B200x1 NVFP4 Throughput
      ngcMetadata:
        78fdabce8c3eae38cea72ca3f28aaca02e3cc475c17913d6e8d4e554cba2aaa9:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: d07936f83cf22e053e9fe3339050f2e05459ebcde766c94fdb7a6ac90aeb1fda
            number_of_gpus: '2'
            pp: '1'
            precision: nvfp4
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: B200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 6GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct GH200_480GBx1 FP8 Latency
      ngcMetadata:
        7c71f0d6db2e0d52a3fbc34dabd0584ed7a27ef63a49e21aaa394d8746eeb189:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: GH200_480GB
            gpu_device: 2342:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 1fa738c9de9d4b25a298b3cd021b05beab57c2c9ab5a930a3d1efcf7204fc463
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GH200_480GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2342:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:h200x1-throughput-fp8-mqkoo4u9fa
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H200x1 FP8 Throughput
      ngcMetadata:
        83fa1ce989c823d1fba445823ac58beb734bb31383a33af261a8b0808495678a:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H200
            gpu_device: 2335:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: d7e3e88abee403b365d238441ca9c1172e71745fe43cd7dce511e7d95309d237
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2335:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 9GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct RTX6000_BLACKWELL_SVx1 NVFP4 Throughput
      ngcMetadata:
        882e2041a947f6e0793a600a4470fbbd41e7a3f3363bb4956a2c63aaa7cf51ec:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 0852d2d4b6d54d6bc12acf922890bfa19e801e74276ddddcff98034ba0dc4c0f
            number_of_gpus: '2'
            pp: '1'
            precision: nvfp4
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct RTX6000_BLACKWELL_SVx1 FP8 Latency
      ngcMetadata:
        8855de19ef9d0f55c0213a8786591091cc5965a2c862562cc7b492c712ef09e3:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: a11707b8479a7230d31a451c07c5650f0e8ff58948507a983a2e89b846929ecf
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H200_NVLx2 BF16 Latency
      ngcMetadata:
        885fb853c59fc5ea3a61554797670d6f61e4b2db23f1acbc69f7e8e98846ce21:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H200_NVL
            gpu_device: 233b:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 362023b28455913264302e9d87593459bc9930c544859af88689346e92085fea
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H200_NVL
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 233B:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct A100_SXM4_40GBx2 BF16 Latency
      ngcMetadata:
        88b3c4d52c48162915703053126fe2d2ec64632b4508fb05dd0984904cc4b313:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: A100_SXM4_40GB
            gpu_device: 20b0:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 344bc1f6c75518604e27015bf9131a6dc8c5257396806f35575516bb14234706
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A100_SXM4_40GB
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 20B0:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:gb200x1-throughput-bf16-qsrhtlj33g
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct GB200x1 BF16 Throughput
      ngcMetadata:
        8a33858f5392a45aa85acaab0a81601e9831cfd99507249536c63be228f09918:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 3cb77524c717d774efbda1f850840b59abc39d9bd46fc2983ebc3dc1f4931ff6
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: GB200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 16GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H200_NVLx2 FP8 Latency
      ngcMetadata:
        8b0cd9578c1bf872d35c8da2dc72ed6f2161623840923884a8f50725ec11a4ec:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H200_NVL
            gpu_device: 233b:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 3a251730a9c214eea8101d8192f6c4c35b1d321aad615edc1e0a942521b828b0
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H200_NVL
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 233B:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:h100x2-latency-bf16-xhazfvu8og
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H100x2 BF16 Latency
      ngcMetadata:
        8ecf55cfb8e611fb1e1579b57089060c76270bcafb322a872c751cb59ba840bc:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 782d96856a10dac93438804c42286ba3e7d0d7445d7fbd8f8497dd3c80238564
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H100
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2330:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 17GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:h100x1-throughput-bf16-tgzhmf3syg
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H100x1 BF16 Throughput
      ngcMetadata:
        90061152a480ade6c471a982258bf4e42dc51cf29ad9f6642120547c33bdf51f:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 04c9939245eef94e92510642382d6ad26f65a25cf1687b76ccc4e66aba70da39
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H100
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2330:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 16GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct GH200_144GBx1 BF16 Throughput
      ngcMetadata:
        9020f539c475f53d364474485cd83728454b7a340c0f1ee2d3cf505ccdcc1189:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: GH200_144GB
            gpu_device: 2348:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: ca7f6e7d29a1f514a9d7f4f3d731b0a0d286a9358a96287b12a1045ac9ca590b
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: GH200_144GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2348:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:b200x2-latency-fp8-hrzafixo7g
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct B200x2 FP8 Latency
      ngcMetadata:
        94420d0c4e672e70e91c15d5a6e23c447fa3b43f1632936eebf9cdd0c845d036:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: b1e67d29794a75bf923a7224dd297dbc4aacd4d97273a7fd66dda7e8371a6da8
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: B200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 9GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct RTX6000_BLACKWELL_SVx1 NVFP4 Latency
      ngcMetadata:
        95f587f27ab8c1467d93d12ffb7db8f3920888b4211c2ab82ef4f8de2fca61f5:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 9cd258950837575782b24640b90c1cd969334d691131036208cd3c8b0735f927
            number_of_gpus: '2'
            pp: '1'
            precision: nvfp4
            profile: latency
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H200_NVLx1 FP8 Throughput
      ngcMetadata:
        9b0e99f6e9afa6fa529d47662d85b1e6d16b3abadcc2a5e72c10486eb7c87201:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H200_NVL
            gpu_device: 233b:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 93297aebf65f337e982d4ebd8e79f380bd9ad05346cf2e18908c6365de2b2307
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H200_NVL
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 233B:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:gb200x2-latency-nvfp4-sgvjjrbeuw
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct GB200x2 NVFP4 Latency
      ngcMetadata:
        9f558e6681791166fdc01cacf06f2d869b67c26f1d573738f92e5e227f820270:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: efc59c490dab28006a89a5d53a36d4ef5c5d3b0927c7d118f02014d4eb0c29e8
            number_of_gpus: '4'
            pp: '1'
            precision: nvfp4
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: GB200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 6GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:l40sx1-throughput-fp8-xad-wr2scw
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct L40Sx1 FP8 Throughput
      ngcMetadata:
        a3e90cba8e03efc80877da3902607362c851c36e8c45cd92aada9e7cac900765:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: L40S
            gpu_device: 26b9:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 7ebba124e3f58d0563b96377f7c85432ef3e2f393efb9e158fd308a8738abcfb
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: L40S
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 26B9:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 9GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:h200x2-latency-fp8-cftgwz2fda
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H200x2 FP8 Latency
      ngcMetadata:
        a826d9d8199abbe4e4084a2f64d3658ef6749b1697ecf21fd0615d1e138e368d:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H200
            gpu_device: 2335:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 6bc839be18669cae90a69af4e02503965be03b8c68b1b7ac2cf6b612033abeb8
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2335:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 9GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:a100x2-latency-bf16-oxfjg8md-a
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct A100x2 BF16 Latency
      ngcMetadata:
        ad582d87e490e749edcaf041d763e6c3f492962ccdbbe83e9204b48d6cfe7641:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: A100
            gpu_device: 20b2:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 0b1583a74d6516dd30c0bfdce8972835384f10fb4f617df4e0260fdf5092b059
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A100
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 20B2:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 17GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:gb200x1-throughput-nvfp4-ihgvv-o6wg
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct GB200x1 NVFP4 Throughput
      ngcMetadata:
        adbc8a19059852df0c2ac75173b80f123b0901926e524a2c050ce60aa3ae5ca1:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: a381f8cc9d090bc49cb320a47cdefe01b0555dc9409312516194cb19437436d0
            number_of_gpus: '2'
            pp: '1'
            precision: nvfp4
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: GB200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 6GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H100_NVLx2 BF16 Latency
      ngcMetadata:
        afc6d2a8f5c1affe8524a39c78d6f083cd56ac678f9cc9f89df33b0e0e530ec5:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H100_NVL
            gpu_device: 2321:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: d0040f5636dd8d4baa9c36337ccb4157b58e47ba7118b2d54b36e2ad96061ed0
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H100_NVL
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2321:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct GH200_144GBx1 FP8 Throughput
      ngcMetadata:
        b0c4bcf92286ad2f689805bf411e44a617df5a5455c703ddd8053f354d40b5cb:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: GH200_144GB
            gpu_device: 2348:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 0ec33e185e2d8e2e4bd97118f54276dbece4b6744371f895bd4c86e8e4dceedb
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GH200_144GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2348:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:h200x1-throughput-bf16-6ylo-i-bbw
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H200x1 BF16 Throughput
      ngcMetadata:
        b795f66a018d1278aaded769cb88a79b5565d2fe6497739b03d8f1bad88e75d1:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H200
            gpu_device: 2335:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 673a0a6cdd37cabec7d8dcc8f05f787884c72f2b56fcaad416429dda38238c0b
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2335:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 16GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct GH200_480GBx1 BF16 Latency
      ngcMetadata:
        c11d003373b87576201557974186967205684e4045905b5140a3d92f274cbf5f:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: GH200_480GB
            gpu_device: 2342:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: b41e403000a7d5221cdd4f00ab4d8a2ef58aa3470a65db51abd523b245c63ea6
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: GH200_480GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2342:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:l40sx2-latency-fp8-szl6-yje2g
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct L40Sx2 FP8 Latency
      ngcMetadata:
        c512ff489822b14e13879c4b1cbb849e5a45d453beeb2d9abfe52f029c0639d2:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: L40S
            gpu_device: 26b9:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 570d33681085a88ae5ea6bd28342996817acb2d9b0a5e8486e197bba77b832a1
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: L40S
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 26B9:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 9GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct RTX6000_ADAx1 INT4_AWQ Latency
      ngcMetadata:
        c95bbf72a36cc53dd0750074c0307cbc16ef98a8634cd89f94046e226c892ac9:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: RTX6000_ADA
            gpu_device: 26b1:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 737909c201e9ccea9bfb138401ed768b71985f2aad636ed91b7ca0712e02cb43
            number_of_gpus: '1'
            pp: '1'
            precision: int4_awq
            profile: latency
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: INT4_AWQ
      - key: GPU
        value: RTX6000_ADA
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 26B1:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct RTX6000_BLACKWELL_SVx1 BF16 Latency
      ngcMetadata:
        ccfeded811dbe0f17d70c25f83c247d1317114349b5df99ba1044c1fcb79b8ef:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 689cc0a0026ff1fff0e5818d26ffe369c59e7b16f96d9239248504fbab23c28c
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct RTX6000_BLACKWELL_SVx1 BF16 Throughput
      ngcMetadata:
        d3ab627cccb5910fbce6396c9d205c84792abee634eb9f334c47086cf5d01b12:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: fa369969c2e20bb29b6b004cde3f63ba17a65056818bd8ad63528141ecb41527
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:a100x1-throughput-bf16-lwcrbwztpq
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct A100x1 BF16 Throughput
      ngcMetadata:
        d67b7f59a9a2851e98bce877ee3702e82a3166322418dbf900a6a15e46643472:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: A100
            gpu_device: 20b2:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 2c735fe09841c686ba2f7ace400337d0f11be549d41cdeb9ba1c82688d0688fa
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A100
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 20B2:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 16GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct GH200_144GBx1 BF16 Latency
      ngcMetadata:
        db8a6f9d6f65eaec69ec78ea131cb34ec66bc63df975d23f8d2ccb031806dcc8:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: GH200_144GB
            gpu_device: 2348:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: c9d911dd01c8c520784ca0fec350f83855a8ca1ea1a3fed5f707fa642945a3e3
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: GH200_144GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2348:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:b200x1-throughput-fp8-i5rbiys4jq
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct B200x1 FP8 Throughput
      ngcMetadata:
        e0b3ee6ce141beca50c67daccbebb1ce7417c14acd08c81346986898042733b6:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 9d7c4bc4201757dcd3f1147712dfb1c83a8f8535405a59e1fa547f17b4a5869b
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: B200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 9GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct H100_NVLx1 BF16 Throughput
      ngcMetadata:
        e6c81e90a8ff3f2cf1b1bffbf760b05c7cf12d18c6486a4690b8ab81b6de436d:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: H100_NVL
            gpu_device: 2321:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: b75eb29f401f555ea3d19a6df30861ef874d10760590ee8856ff0542d7ea1e7f
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H100_NVL
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2321:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct GH200_480GBx1 FP8 Throughput
      ngcMetadata:
        ed144c17645499c4cd983b4a2e4bdc23f0f03cc55e19073c357e8eb0ff982dc6:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: GH200_480GB
            gpu_device: 2342:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: aab7bbbbcf3e6f7fb544ee77017dd2ee59aa0b81dc314dec4bb46317def34714
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GH200_480GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2342:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct RTX6000_BLACKWELL_SVx1 FP8 Throughput
      ngcMetadata:
        ee928087f01a5df571cf5e62c96f66fedccaf180524ce1e43cb4b5a23295deb8:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 60b75ec923fe8a417deb0273d9a21b4fcd4e3e0f8f9ed6e9527a66433cf6030c
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:l40sx1-throughput-bf16-lh60z9g-aq
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct L40Sx1 BF16 Throughput
      ngcMetadata:
        eece8dae913d9055ed8060b6ae1764cefecd6d158dd314851e1ecd15b5d9126d:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: L40S
            gpu_device: 26b9:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: ded6e827825103665f5fea2381f04196a88c95525aea31c073556f0204ad9c8e
            number_of_gpus: '1'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: L40S
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 26B9:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 16GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:8c22764a7e3675c50d4c7c9a4edb474456022b16
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct GH200_144GBx1 FP8 Latency
      ngcMetadata:
        ef8d429a394978d394a8d15ddbdd6666bde4dc68e40f8cb399b188f5b7e59db5:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: GH200_144GB
            gpu_device: 2348:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 02d2b0459a15ab0a39ec4335caf74e3105e66f51ae577b7bf8a1b64cfcb5c472
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GH200_144GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2348:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 30GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-8b-instruct:a10gx4-throughput-bf16-g04kznyzwa
      framework: TensorRT-LLM
      displayName: Llama 3.1 8B Instruct A10Gx4 BF16 Throughput
      ngcMetadata:
        f02876a90b3197bcf046fee9ab2beb6f7482b8b35e3ff9ff545d03ba9ba7bb23:
          model: meta/llama-3.1-8b-instruct
          release: 1.13.1
          tags:
            feat_lora: 'false'
            gpu: A10G
            gpu_device: 2237:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: fda7bc4c0bf1f5eaeecbf29fbdd078ecbf587ac57c62610180d3d5fb90ffcfda
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A10G
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2237:10DE
      - key: NIM VERSION
        value: 1.13.1
      - key: DOWNLOAD SIZE
        value: 19GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
  - variantId: Llama 3.1 70B Instruct
    modelCard: {
    "accessType": "NOT_LISTED",
    "application": "Other",
    "bias": "",
    "canGuestDownload": false,
    "createdDate": "2025-05-20T18:17:09.107Z",
    "description": "# **Llama-3.1-70B-Instruct Overview**\n\n## **Description:**\n\n**Llama-3.1-70B-Instruct** is a multilingual large language model from the Meta Llama 3.1 collection of pretrained and instruction-tuned generative models. This model is optimized for multilingual dialogue use cases and outperforms many available open-source and closed-source chat models on common industry benchmarks. Llama 3.1 is an auto-regressive language model that uses an optimized transformer architecture; the tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety.\n\nThis model is ready for commercial/non-commercial use.\n\nThis version introduces support for GB200 NVL72, GH200 NVL2, B200 and NVFP4. CUDA updated to version 12.9. For detailed information, refer to Release [Notes for NVIDIA NIM for LLMs LLM 1.12](https://docs.nvidia.com/nim/large-language-models/latest/release-notes.html). \n\n## **Third-Party Community Consideration**\n\nThis model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIA\\[meta-llama/Llama-3.1-70B-Instruct\\]  \n([https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct)). \n\n## **License/Terms of Use:**\n\n**GOVERNING TERMS:** The NIM container is governed by the [NVIDIA Software License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) and the [Product-Specific Terms for NVIDIA AI Products](https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/); and the use of the model is governed by the [NVIDIA Community Model License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-community-models-license/.).\n\n**ADDITIONAL INFORMATION:** [Llama 3.1 Community License Agreement](https://www.llama.com/llama3_3/license/). Built with Llama.\n\n## Get Help\n\n### Enterprise Support\n\nGet access to knowledge base articles and support cases or [submit a ticket](https://www.nvidia.com/en-us/data-center/products/ai-enterprise-suite/support/).\n\n**You are responsible for ensuring that your use of NVIDIA provided models complies with all applicable laws.**\n\n## **Deployment Geography:**\n\nGlobal \n\n## **Use Case:**\n\nDevelopers, AI researchers, and businesses would be expected to use this system to build and power a wide range of applications that require advanced reasoning, instruction-following, and multilingual dialogue capabilities. Specific applications include creating sophisticated chatbots and virtual assistants, developing powerful content creation and summarization tools, building complex question-answering systems, and powering multilingual customer support platforms.\n\n## **Release Date:**\n\nBuild.Nvidia.com 07/23/2024 via  \n[llama-3.1-70b-instruct Model by Meta | NVIDIA NIM](https://build.nvidia.com/meta/llama-3_1-70b-instruct)\n\nGithub 07/23/2024 via   \n[https://github.com/meta-llama/llama-models/blob/main/models/llama3\\_1/LICENSE](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)\n\nHuggingface 07/23/2024 via   \n[https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct)\n\n**Reference(s):** \n\n[https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct)\n\n## **Model Architecture:** \n\nArchitecture Type: Transformer  \nNetwork Architecture: Llama-3.1-70B\n\nThis model was developed based on Meta-Llama-3.1-70B  \n[https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct)\n\nNumber of model parameters: 7.06*10^10\n\n## **Input:**\n\nInput Type(s): Text \n\nInput Format(s): String \n\nInput Parameters: One-Dimensional (1D)\n\nOther Properties Related to Input: The model accepts a string of text which is converted into tokens using the model's specific tokenizer. The total length of the input prompt and the generated output cannot exceed the model's context window of 128,000 tokens.\n\n## **Output:**\n\nOutput Type(s): Text \n\nOutput Format(s): String\n\nOutput Parameters: One-Dimensional (1D)\n\nOther Properties Related to Output: The model generates a string of text, produced token by token. The maximum length of the output is limited by the model's 128,000-token context window, less the length of the input prompt. Post-processing is required to decode the model's token-based output into a readable string.\n\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\n\n## **Software Integration:**\n\nRuntime Engine: vLLM, TensorRT\n\nSupported Hardware Microarchitecture Compatibility:\n\nNVIDIA Ampere  \nNVIDIA Blackwell  \nNVIDIA Hopper  \nNVIDIA Lovelace \n\nPreferred Operating System(s):\n\nLinux   \nWindows\n\nThe integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment.\n\n## **Model Version(s):**\n\nLlama-3.1-70B-Instruct\n\n## **Usage**\n\n**Use with transformers**\n\nStarting with transformers \\>= 4.45.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\n\nMake sure to update your transformers installation via pip install \\--upgrade transformers.\n\nSee the snippet below for usage with Transformers:\n\n```\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\n**Tool use with transformers**\n\nLLaMA-3.3 supports multiple tool use formats. You can see a full guide to prompt formatting [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/).\n\nTool use is also supported through [chat templates](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling) in Transformers. Here is a quick example showing a single simple tool:\n\n```\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n    \"\"\"\n    Get the current temperature at a location.\n    \n    Args:\n        location: The location to get the temperature for, in the format \"City, Country\"\n    Returns:\n        The current temperature at the specified location in the specified units, as a float.\n    \"\"\"\n    return 22.  # A real function should probably actually get the temperature!\n\n# Next, create a chat and apply the chat template\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\n```\n\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\n\n```\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n```\n\nand then call the tool and append the result, with the tool role, like so:\n\n```\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n```\n\nAfter that, you can generate() again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling \\- for more information, see the [LLaMA prompt format docs](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/) and the Transformers [tool use documentation](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling).\n\n**Use with bitsandbytes**\n\nThe model checkpoints can be used in 8-bit and 4-bit for further memory optimisations using bitsandbytes and transformers\n\nSee the snippet below for usage:\n\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\nquantized_model = AutoModelForCausalLM.from_pretrained(\n    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ninput_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutput = quantized_model.generate(**input_ids, max_new_tokens=10)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nTo load in 4-bit simply pass load\\_in\\_4bit=True\n\n**Use with llama**\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama).\n\nTo download Original checkpoints, see the example command below leveraging huggingface-cli:\n\n```\nhuggingface-cli download meta-llama/Meta-Llama-3.1-70B-Instruct --include \"original/*\" --local-dir Meta-Llama-3.1-70B-Instruct\n```\n\n## **Training, Testing, and Evaluation Datasets:**\n\n### **Training Dataset**\n\n**Data Modality:** Text \n\n**Link:** Undisclosed\n\n**Data Collection Method:** Hybrid: Human, Synthetic, Automated\n\n**Labeling Method:** Hybrid: Automated, Human\n\n**Properties:** \n\nThe pretraining dataset contains over 15 trillion tokens. It is a multilingual dataset covering over 30 languages and was filtered heavily for quality using various techniques, including heuristic filters, NSFW filters, and text classifiers. The model's knowledge was trained on data with a cutoff of December 2023\\. \n\n### **Testing Dataset**\n\n**Link:** Undisclosed\n\n**Data Collection Method:** Hybrid: Human, Synthetic, Automated\n\n**Labeling Method:** Hybrid: Automated, Human\n\n**Properties:** \n\n**Description:**\n\nThe model was tested on a diverse set of evaluation data.\n\n* Public Benchmarks: These test a wide range of capabilities, from general knowledge and reasoning (MMLU, HellaSwag) to expert-level problem-solving (GPQA) and programming (HumanEval, which contains 164 programming problems).  \n* Internal Evaluation Set: Meta created a new high-quality test set of 2,000 prompts covering 12 key use cases (e.g., coding, reasoning, creative writing, instruction following). This set is used for human evaluation to assess performance on real-world, nuanced tasks. \n\n### **Evaluation Dataset**\n\n**Link:** Undisclosed\n\n**Data Collection Method:** Hybrid: Automated, Human, Synthetic\n\n**Labeling Method:** Hybrid: Human, Automated\n\n**Properties:** \n\nThe evaluation datasets are diverse and test a wide spectrum of capabilities. MMLU measures broad multitask knowledge. GPQA assesses advanced reasoning with difficult, expert-level questions. HumanEval and MATH specifically test code generation and mathematical reasoning abilities, respectively. Meta also utilizes a large, private, human-annotated evaluation set designed to assess model performance in real-world, nuanced scenarios. \n\n**Base pretrained models**\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3 8B | Llama 3.1 8B | Llama 3 70B | Llama 3.1 70B | Llama 3.1 405B |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| General | MMLU | 5 | macro\\_avg/acc\\_char | 66.7 | 66.7 | 79.5 | 79.3 | 85.2 |\n|  | MMLU-Pro (CoT) | 5 | macro\\_avg/acc\\_char | 36.2 | 37.1 | 55.0 | 53.8 | 61.6 |\n|  | AGIEval English | 3-5 | average/acc\\_char | 47.1 | 47.8 | 63.0 | 64.6 | 71.6 |\n|  | CommonSenseQA | 7 | acc\\_char | 72.6 | 75.0 | 83.8 | 84.1 | 85.8 |\n|  | Winogrande | 5 | acc\\_char | \\- | 60.5 | \\- | 83.3 | 86.7 |\n|  | BIG-Bench Hard (CoT) | 3 | average/em | 61.1 | 64.2 | 81.3 | 81.6 | 85.9 |\n|  | ARC-Challenge | 25 | acc\\_char | 79.4 | 79.7 | 93.1 | 92.9 | 96.1 |\n| Knowledge reasoning | TriviaQA-Wiki | 5 | em | 78.5 | 77.6 | 89.7 | 89.8 | 91.8 |\n| Reading comprehension | SQuAD | 1 | em | 76.4 | 77.0 | 85.6 | 81.8 | 89.3 |\n|  | QuAC (F1) | 1 | f1 | 44.4 | 44.9 | 51.1 | 51.1 | 53.6 |\n|  | BoolQ | 0 | acc\\_char | 75.7 | 75.0 | 79.0 | 79.4 | 80.0 |\n|  | DROP (F1) | 3 | f1 | 58.4 | 59.5 | 79.7 | 79.6 | 84.8 |\n\n**Instruction tuned models**\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3 8B Instruct | Llama 3.1 8B Instruct | Llama 3 70B Instruct | Llama 3.1 70B Instruct | Llama 3.1 405B Instruct |\n| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |\n| General | MMLU | 5 | macro\\_avg/acc | 68.5 | 69.4 | 82.0 | 83.6 | 87.3 |\n|  | MMLU (CoT) | 0 | macro\\_avg/acc | 65.3 | 73.0 | 80.9 | 86.0 | 88.6 |\n|  | MMLU-Pro (CoT) | 5 | micro\\_avg/acc\\_char | 45.5 | 48.3 | 63.4 | 66.4 | 73.3 |\n|  | IFEval |  |  | 76.8 | 80.4 | 82.9 | 87.5 | 88.6 |\n| Reasoning | ARC-C | 0 | acc | 82.4 | 83.4 | 94.4 | 94.8 | 96.9 |\n|  | GPQA | 0 | em | 34.6 | 30.4 | 39.5 | 46.7 | 50.7 |\n| Code | HumanEval | 0 | pass@1 | 60.4 | 72.6 | 81.7 | 80.5 | 89.0 |\n|  | MBPP \\++ base version | 0 | pass@1 | 70.6 | 72.8 | 82.5 | 86.0 | 88.6 |\n|  | Multipl-E HumanEval | 0 | pass@1 | \\- | 50.8 | \\- | 65.5 | 75.2 |\n|  | Multipl-E MBPP | 0 | pass@1 | \\- | 52.4 | \\- | 62.0 | 65.7 |\n| Math | GSM-8K (CoT) | 8 | em\\_maj1@1 | 80.6 | 84.5 | 93.0 | 95.1 | 96.8 |\n|  | MATH (CoT) | 0 | final\\_em | 29.1 | 51.9 | 51.0 | 68.0 | 73.8 |\n| Tool Use | API-Bank | 0 | acc | 48.3 | 82.6 | 85.1 | 90.0 | 92.0 |\n|  | BFCL | 0 | acc | 60.3 | 76.1 | 83.0 | 84.8 | 88.5 |\n|  | Gorilla Benchmark API Bench | 0 | acc | 1.7 | 8.2 | 14.7 | 29.7 | 35.3 |\n|  | Nexus (0-shot) | 0 | macro\\_avg/acc | 18.1 | 38.5 | 47.8 | 56.7 | 58.7 |\n| Multilingual | Multilingual MGSM (CoT) | 0 | em | \\- | 68.9 | \\- | 86.9 | 91.6 |\n\n**Multilingual benchmarks**\n\n| Category | Benchmark | Language | Llama 3.1 8B | Llama 3.1 70B | Llama 3.1 405B |\n| :---- | :---- | :---- | :---- | :---- | :---- |\n| General | MMLU (5-shot, macro\\_avg/acc) | Portuguese | 62.12 | 80.13 | 84.95 |\n|  |  | Spanish | 62.45 | 80.05 | 85.08 |\n|  |  | Italian | 61.63 | 80.4 | 85.04 |\n|  |  | German | 60.59 | 79.27 | 84.36 |\n|  |  | French | 62.34 | 79.82 | 84.66 |\n|  |  | Hindi | 50.88 | 74.52 | 80.31 |\n|  |  | Thai | 50.32 | 72.95 | 78.21 |\n\n## **Technical Limitations** \n\nTesting conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, the model's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying this model in any applications, developers should perform safety testing and tuning tailored to their specific applications. Please refer to available resources including the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development. \n\n## **Inference:**\n\n**Acceleration Engine:** vLLM, TensorRT \n\n**Test Hardware:** \n\n  B200 SXM   \n  H200 SXM  \n  H100 SXM  \n  A100 SXM 80GB  \n  A100 SXM 40GB  \n  L40S PCIe  \n  A10G  \n  H100 NVL  \n  H200 NVL  \n  GH200 96GB  \n  GB200 NVL72   \n  GH200 NVL2     \n  RTX 5090  \n  RTX 4090  \n  RTX 6000 Ada\n\n## **Ethical Considerations:**\n\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\nYou are responsible for ensuring that your use of NVIDIA provided models complies with all applicable laws.",
    "displayName": "Llama-3.1-70b-instruct",
    "explainability": "",
    "framework": "Other",
    "hasPlayground": false,
    "hasSignedVersion": true,
    "isPlaygroundEnabled": false,
    "isPublic": false,
    "isReadOnly": true,
    "labels": [
        "Llama3.1",
        "Llama3.1-70b-instruct",
        "NIM",
        "NSPECT-7S3F-QFG8",
        "nvaie:model:nvaie_supported",
        "nvidia_nim:model:nimmcro_nvidia_nim",
        "productNames:nim-dev",
        "productNames:nv-ai-enterprise"
    ],
    "latestVersionIdStr": "rtx6000-blackwell-svx8-latency-bf16-bxiagh4jgg",
    "latestVersionSizeInBytes": 157357267927,
    "logo": "https://assets.ngc.nvidia.com/products/api-catalog/images/llama-3_1-70b-instruct.jpg",
    "modelFormat": "SavedModel",
    "name": "llama-3.1-70b-instruct",
    "orgName": "nim",
    "precision": "OTHER",
    "privacy": "",
    "productNames": [
        "nim-dev",
        "nv-ai-enterprise"
    ],
    "publicDatasetUsed": {},
    "publisher": "Meta",
    "safetyAndSecurity": "",
    "shortDescription": "The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out).",
    "teamName": "meta",
    "updatedDate": "2025-10-15T17:49:15.605Z"
}
    source:
      URL: https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/llama-3_1-70b-instruct-nemo
    optimizationProfiles:
    - profileId: nim/meta/llama-3.1-70b-instruct:b200x1-throughput-nvfp4-lissxvpltg
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct B200x1 NVFP4 Throughput
      ngcMetadata:
        1b7ebc7f2cd12aa502b3f2bc17fa55a91f304abd992b287c535a59b6536d3e05:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: b476c975e5339b67e01a1a9aee137aa1dd80c1d520b62ba160b64e426c8e2e6e
            number_of_gpus: '1'
            pp: '1'
            precision: nvfp4
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: B200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 41GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct RTX6000_BLACKWELL_SVx8 BF16 Latency
      ngcMetadata:
        266a5944d595ad57b186c01686b30ba7d1fc10f22a5b4fa17ef8d5cd54faf0f8:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 62c6e2eeff50dd4b71f6a31817eed7685778f8d1415340f402e269add0ca102b
            number_of_gpus: '8'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '8'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:h200x4-latency-bf16-csp1xgtxoq
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct H200x4 BF16 Latency
      ngcMetadata:
        2a56d7a6042e02c5b469f5128c76379973e255caf5b1adc1cde6e03230159077:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H200
            gpu_device: 2335:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 91f8367eac71f0e5731988bac7b8b9ae66747619ed7cea336ff1ad2609b07945
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H200
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2335:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 139GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct A100_SXM4_40GBx8 BF16 Throughput
      ngcMetadata:
        2fdeceaf1b64acf3ab1c2a22b8e23f6c25d639d6a5d7006c51c80b613fb2699b:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: A100_SXM4_40GB
            gpu_device: 20b0:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: ce61710173c15471c4031430bc8de32b94fb1859a9d4d4cced5c09664b9658c3
            number_of_gpus: '8'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '8'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A100_SXM4_40GB
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 20B0:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:l40sx4-throughput-fp8-ulen5raong
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct L40Sx4 FP8 Throughput
      ngcMetadata:
        3013dcf9b905cbd2f5e23f804fd5d66d183ddc71a8735631d3cad277f7c23897:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: L40S
            gpu_device: 26b9:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: ce87554d33c9d66d40c52c15b3a90b5c802ef4b7d05781dc74fd18485a20e15d
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: L40S
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 26B9:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 69GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:gb200x1-latency-nvfp4-aiiz15cu0w
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct GB200x1 NVFP4 Latency
      ngcMetadata:
        344979e57f70e669d35378bc48ef7d14a13dc6aa0467ce9cb29166b8a8371bcb:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: b668500698d48c5aee9f5b591c4383cb62053acb59539cf0b511b8b2d2ae864f
            number_of_gpus: '1'
            pp: '1'
            precision: nvfp4
            profile: latency
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: GB200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 41GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct RTX6000_BLACKWELL_SVx2 FP8 Throughput
      ngcMetadata:
        3526ceaf332ec21d4317c0939a99a3862b19593527fa942ffd5a1df2dade47ce:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 2a535c9c9ddfb8e328abc28f3b4d9564ecdf9886fa177096f7b38dee7af754ab
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct H100_NVLx2 BF16 Throughput
      ngcMetadata:
        3684471ad5d007fa1f72bbc672a794107de7b0e8df88214dc1563a24aa99c8b7:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100_NVL
            gpu_device: 2321:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 22af385c5fd7064b011e826d0d78c210b7ac1fe7a9e29eef15e6a5e433b9db9d
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H100_NVL
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2321:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:b200x2-latency-nvfp4-hrt0sgzswa
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct B200x2 NVFP4 Latency
      ngcMetadata:
        377c705c5682293482c5094b946b8e74ccba5302c324b5ce41f952e9cac29890:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 289f1c679cc71fbadaa8139366458b0c3fc39d49ba067efdb7db9fbf3801ac1c
            number_of_gpus: '2'
            pp: '1'
            precision: nvfp4
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: B200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 41GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:l40sx4-latency-fp8-ctp-cvrc0w
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct L40Sx4 FP8 Latency
      ngcMetadata:
        43160a1132063bf60ef6d7fe17a9b271f03dedbdb3bd1584a2e53707c8faa9ce:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: L40S
            gpu_device: 26b9:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: f1a3de57c511586b258f58e7457103c919f8fa4db289d37961cad2468596ee6c
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: L40S
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 26B9:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 69GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct H100_NVLx2 FP8 Throughput
      ngcMetadata:
        44d44ef91639f0c76a1ef4be0022651ed8d42b485c26de00ba99aee570d1768d:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100_NVL
            gpu_device: 2321:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 1d049541d40b0b0407983f0438189a5d21af6652866d6640437e0323c7878361
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100_NVL
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2321:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:gb200x2-throughput-bf16-lo9t8i-qua
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct GB200x2 BF16 Throughput
      ngcMetadata:
        45c52f130d8d467fa6e91f4ffee683fff5601e16df41388d4047e63e294e1165:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 706ae15947d58ed243812620f46199e223e7288c6624ccd33d9e9393a7bfb96a
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: GB200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 134GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct RTX6000_BLACKWELL_SVx2 NVFP4 Throughput
      ngcMetadata:
        4b9618100e94fc85d674a89eae960e18d8192163abe5db2a0d2be891d32ea06a:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 54ad694d948a6bd8d413341c0d9476b3756a5553aa8ce8ba5479d3b3cf289e9d
            number_of_gpus: '2'
            pp: '1'
            precision: nvfp4
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:h100x4-throughput-bf16-wf01-bcefa
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct H100x4 BF16 Throughput
      ngcMetadata:
        4d9f79288ba78fd61b3cc445c6f9da30362a132ea371798a8ec3dff7bddc3a20:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 22f873ff61f22bd360dc173f0f4a068d4d950c02ea6045570eb7f50ec8f83e93
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H100
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2330:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 139GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:h100x2-throughput-fp8-vjxy5bkroq
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct H100x2 FP8 Throughput
      ngcMetadata:
        4fbe63c3f6f9b928dac05fe81a278ac1ad45ccf329850f66bd6cbc0c2f2c044c:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: bb98ff9885aa439391b057063cce3555833a27f88d982b2c210fd4b752390475
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2330:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 69GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct GH200_144GBx2 BF16 Latency
      ngcMetadata:
        50fe6d2879cabe91e1e0b96314d40695e7ffc9e83a02d63629b9cabfae496dbe:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GH200_144GB
            gpu_device: 2348:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 21c7fedc20e7b94738606f7f4f8ebb346dc3f087f082dd32b713e4b8e6ed0a06
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: GH200_144GB
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2348:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct H200_NVLx2 FP8 Latency
      ngcMetadata:
        592714cb05c8f25c0445fb7467d096956db9bfbee0958eb713c02a5410867bff:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H200_NVL
            gpu_device: 233b:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 90dae25f9f80d311b10f61f5772c37bac723422cce689c138396be49db0b82f4
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H200_NVL
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 233B:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct GH200_144GBx1 FP8 Throughput
      ngcMetadata:
        5fbbcbeb676751bfdc9b65cca39334f82fbe543070ea66b4756f71de6cfe2b59:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GH200_144GB
            gpu_device: 2348:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: b2d2411595259e3d02add53ce15aaae59cf5bb02731910aecbd8b5b7a3f75adc
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GH200_144GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2348:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:a100x4-throughput-bf16-ftwaepe7oq
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct A100x4 BF16 Throughput
      ngcMetadata:
        68aff19a2e5198624143bf25060662c863ecf21039b6f2d4ef3fe7965a8bab96:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: A100
            gpu_device: 20b2:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 8f849c4baf82033a3bbfba75bd2a6fc379c92079e81f6fca99f978c9d1c04ad1
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A100
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 20B2:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 140GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:h200x2-latency-fp8-msyzoyixrw
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct H200x2 FP8 Latency
      ngcMetadata:
        6c27932dc47820a7130505d6bceca05a3ec27628a8416b4603b9b9c8367f161d:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H200
            gpu_device: 2335:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 070c645e1731e5dd9875c800a22cadcd32f9008bf79b768884a331afc9c96e25
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2335:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 69GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:l40sx4-throughput-bf16-gfrr6smxia
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct L40Sx4 BF16 Throughput
      ngcMetadata:
        6c9c0490830921741f09a61b59d32ff645681d80194b3af37214824d65f05e7e:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: L40S
            gpu_device: 26b9:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: e4710ee55a988086fb6dd511b81e989b78d523d951f0da1719cf0328d750a71e
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: L40S
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 26B9:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 140GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:b200x1-throughput-fp8-ktlniezpyw
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct B200x1 FP8 Throughput
      ngcMetadata:
        741556aa43f38761800674e07ff79f5d61136c8301687b3f914f61c78f72ce46:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: d8ffe1683a73563951753b8b23c0854020887590f3a2112230e0ad947fe1ae99
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: B200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 68GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct GH200_480GBx1 FP8 Throughput
      ngcMetadata:
        75e60d670c274c13e9647548bc1c21549d28871432524a0c86becc2b9c73392e:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GH200_480GB
            gpu_device: 2342:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 6a43b054452f258a2315e308da5d8813a4d5c7672764a4f28218373855853197
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GH200_480GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2342:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct RTX6000_BLACKWELL_SVx4 BF16 Throughput
      ngcMetadata:
        76296a7f2a589f543337824f321c38801835885f3a85d9efc3c5b820d7db5228:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 4f131424adabbfc81a877f09da0ca3bb31989fc0bed618b8d0c5969faa01f7fe
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:a100x8-latency-bf16-zb8ixw2ong
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct A100x8 BF16 Latency
      ngcMetadata:
        7cfa94d868fb7d979659d8418cbf37496cefd480d3b3b3ea06877b08e2868827:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: A100
            gpu_device: 20b2:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 698871a2af3710aa48027caa8536573c057658a99a89b8d9652e15f19f0c2e12
            number_of_gpus: '8'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '8'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A100
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 20B2:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 147GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct GH200_480GBx1 FP8 Latency
      ngcMetadata:
        88a14e8523e8747165e8574a84cee8c4a580af03ced367e74017bf4046835dd2:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GH200_480GB
            gpu_device: 2342:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: e85188dc62e0c517a93bc24a32bee7b7f27b66fa0c6e3184813a4873369e413f
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GH200_480GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2342:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:b200x4-latency-bf16-gqr-l-hprg
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct B200x4 BF16 Latency
      ngcMetadata:
        8cc3eeb4f2ae763b36bf76a67ed42daea7b533852a65090b522440956de4f327:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 3a0889dd10acf4050ccd4fbb878eb5c982c420e3a63df2a2fafaa9fc6c8cf861
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: B200
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 138GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:h100x4-latency-fp8-oxqturnvsg
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct H100x4 FP8 Latency
      ngcMetadata:
        9078b6b41878fcfd7e5e9dca2ea0b5c5560d85d31e2cbb9e0e9801d2bb192bfe:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: cc434557f087f390d162cf972e61958ba9e9f09b6112e174e9824b7bcd92e6f4
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2330:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 69GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct H200_NVLx1 FP8 Throughput
      ngcMetadata:
        92adc0b1a36388246d3f037e68df053c83b4bfe4d23e1fae59f711e6e451b944:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H200_NVL
            gpu_device: 233b:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 0e974238d656d94cb79d39fcc0064f619e6606c678fcba61651358275c693e75
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H200_NVL
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 233B:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:gb200x1-throughput-nvfp4-w75uvvawyq
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct GB200x1 NVFP4 Throughput
      ngcMetadata:
        a92446d9168e5b10aabe4d31889c68b90503f2ee9bbefafa7b406ef1f2f2b92b:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: c593287a9ac875b3a649fb7725a9f1a1e6816129291594a3783a556296bd8808
            number_of_gpus: '1'
            pp: '1'
            precision: nvfp4
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: GB200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 41GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:b200x2-latency-fp8-zkeshhnnug
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct B200x2 FP8 Latency
      ngcMetadata:
        b483bd59b245ec47d9b700691316ed76163f1500d17dcd1fd1fc13ef4fa34dbd:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: daec3db3d904aa6cba1cecd2404867f81d44cf10bb16cc9c9f0ee9a19085bb68
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: B200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 69GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct H100_NVLx4 FP8 Latency
      ngcMetadata:
        b8a18b250c3bd00464dd5194016ecc81756f0121ebc070081bdb2de6dd715a91:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100_NVL
            gpu_device: 2321:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: e741999618e5a4d94b595ff13d17028e5f11db1d5ed50644fd584d34d553198b
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100_NVL
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2321:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct GH200_144GBx2 BF16 Throughput
      ngcMetadata:
        bab01e4b4d692d4d879a405cac30bc3830fb4bfed76deaff130bc989bbf70008:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GH200_144GB
            gpu_device: 2348:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 943e58ca6ffb929366337f69cfc2a49f55a062cb721159a094ae6ade370d2302
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: GH200_144GB
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2348:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:a10gx8-throughput-bf16-c6h2bujzqq
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct A10Gx8 BF16 Throughput
      ngcMetadata:
        bb0acd8d341492a58388d49010ebfd53ccf30e9ba61961e68853b7812bdd57d5:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: A10G
            gpu_device: 2237:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 91af47c6a8cd2c59b5187b47bcb6c3feaef274f685067c0ba391f035ccf265eb
            number_of_gpus: '8'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '8'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A10G
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 2237:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 150GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct GH200_144GBx2 FP8 Latency
      ngcMetadata:
        c02d69cc0542152ece147e75cb33487d9058a83ac94866680455b56c075cede4:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GH200_144GB
            gpu_device: 2348:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 50fd91ee630703f0af954360b638c997fa7e69b60ed949c129e3ba042ed47b66
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GH200_144GB
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2348:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:h200x1-throughput-fp8-j-xwy-p6zg
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct H200x1 FP8 Throughput
      ngcMetadata:
        cb42798192666f9b621fb9a5aeecb342ae389bb6c8992183804aeed016fc1862:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H200
            gpu_device: 2335:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 0a504db21e8269006446b7b777218b5fc904fb8308dd5fbba24de96b577d289f
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2335:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 68GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct GH200_144GBx2 FP8 Throughput
      ngcMetadata:
        d40298dbc0f90c12808e7e5becb22e47c284f05012c73dabb9818f03f461cd10:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GH200_144GB
            gpu_device: 2348:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: ceebc0caeb9bdff847a148e0915219590cc463095bfb9545eb265978e4b8eb81
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GH200_144GB
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2348:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct A100_SXM4_40GBx8 BF16 Latency
      ngcMetadata:
        d847fc6b060db9381d38e6cb59ff183f29c6bb457c402d24c27971e59bad9bf7:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: A100_SXM4_40GB
            gpu_device: 20b0:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 35500bcc312a974be001478a0b1e2466fea9dac13d7bd087146f70d4c9e854c6
            number_of_gpus: '8'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '8'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A100_SXM4_40GB
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 20B0:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:gb200x1-latency-fp8-f3rjvlafrw
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct GB200x1 FP8 Latency
      ngcMetadata:
        dbcdb5f1412398520d7330cb890aa57f1792596f7dc885cc65a1dc20d390cc9d:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: a5a7e9799d92d3e8e2cb39c45acf73dee122f9f65c1bbeaf4eaf7d669745a89c
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GB200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 68GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:l40sx4-latency-bf16-jhyf9rlszq
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct L40Sx4 BF16 Latency
      ngcMetadata:
        e2b3ba60e795d306cf487ea71c8a5d128769f452eeffb54fada6697f031b556c:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: L40S
            gpu_device: 26b9:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 3d65171e66e53a0c6b9d1253a09c393d917cb611ea6de85f7c7abadf7ea934b6
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: L40S
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 26B9:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 138GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct H100_NVLx4 BF16 Latency
      ngcMetadata:
        e6fcaba4b0c11392cd4ce8e0eddc261fac45e994c7735c3d79734245aac1a68d:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100_NVL
            gpu_device: 2321:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: dbc58664a141b59a060de53a9e4f2d25b4ec41f75fcb791a0f046981b0634fad
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H100_NVL
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2321:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:gb200x1-throughput-fp8-trr9koy1vg
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct GB200x1 FP8 Throughput
      ngcMetadata:
        e8e4c9317e3e32c8e50d3f4c54019b40df5608a319359ad9f8257d23f2348c2b:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: cb02e92359903bcfa274d5176c8a7a840e58a3df1587ca0f0c734049d4f1d5c8
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GB200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 68GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:b200x2-throughput-bf16-gbt9zmjfla
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct B200x2 BF16 Throughput
      ngcMetadata:
        f078cbf33438ea9b68c5d4eba7bec671246d44730cbb0af6d94dfa1517bf3036:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 4db4480cbfa2cd40a70ed784e3bb40e3e7e4d6693b7d275fb0b19b328ea1da0a
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: B200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 134GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:a10gx8-latency-bf16-mqqdeavnfg
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct A10Gx8 BF16 Latency
      ngcMetadata:
        f49e065d985faa3a766163f386395cc53c64429754c58cf9edf553ac0ec96244:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: A10G
            gpu_device: 2237:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 9c2fce4ab72d829d5e3b008b9f6b64a608194a97ee6fc18af7863cf922226107
            number_of_gpus: '8'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '8'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A10G
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 2237:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 150GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct RTX6000_BLACKWELL_SVx4 NVFP4 Latency
      ngcMetadata:
        f66f34808a8fe25ee8a3666427569f3e7119b1af54cd64e31d082282d5d47210:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: d7ec938fb438f8d87c58707eb5a60af4e1fb6a9b7ac42eef6738dd9b0d2ff671
            number_of_gpus: '4'
            pp: '1'
            precision: nvfp4
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:h200x2-throughput-bf16-9iwul7vevg
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct H200x2 BF16 Throughput
      ngcMetadata:
        f6d98b286dd43d8a6e677a9a0f218e76928154490a908a2d9f76cbfd2cd043bf:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H200
            gpu_device: 2335:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 583b7cb36984c4bff9e31b3f10937d34d488b2e4e32ac4b0ac5b44e02ef4779b
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2335:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 134GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct RTX6000_BLACKWELL_SVx4 FP8 Latency
      ngcMetadata:
        f7397c4ee54cefd8fc3cc3b947406ba51947215d77ff58f35aeaa298605db13a:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: ead7f13a4122bc78f9624682198bcccbc485345c9a58742601b0bf0e8ed59760
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:1d54af340dc8906a2d21146191a9c184c35e47bd
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct H200_NVLx2 BF16 Throughput
      ngcMetadata:
        fa4fbf5af52b66775f63d40cbc3db263304d7844095d1a677d799b8e90bf141b:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H200_NVL
            gpu_device: 233b:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: d2ae506c5cddf13a3d2f0139dcb6edafd042794999162e7d9623bd4ceabb1b70
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H200_NVL
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 233B:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.1-70b-instruct:gb200x4-latency-bf16-3uozpudciw
      framework: TensorRT-LLM
      displayName: Llama 3.1 70B Instruct GB200x4 BF16 Latency
      ngcMetadata:
        fe20ab9158c65c3e7765e50c5c72ece46ee34a9e184dcdb13eda9bbef78ab300:
          model: meta/llama-3.1-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: e6a18cf6935b817ca2968d06e1abd91444d73634b80ff54f3680d152cadc209e
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: GB200
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 138GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
  labels:
  - Llama
  - Meta
  - Text Generation
  - Large Language Model
  - TensorRT-LLM
  - Language Generation
  - NeMo
  - NVIDIA Validated
  config:
    architectures:
    - Other
    modelType: llama
  license: NVIDIA AI Foundation Models Community License
