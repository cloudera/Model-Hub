models:
- name: Llama 3.3 70B Instruct
  displayName: Llama 3.3 70B Instruct
  modelHubID: llama-3.3-instruct
  category: Text Generation
  type: NGC
  description: The Llama 3.3 70B-Instruct NIM simplifies the deployment of the Llama 3.3 70B instruction tuned model which is optimized for language understanding, reasoning, and text generation use cases, and outperforms many of the available open source chat models on common industry benchmarks.
  requireLicense: true
  licenseAgreements:
  - label: Use Policy
    url: https://www.llama.com/llama3_3/use-policy/
  - label: License Agreement
    url: https://www.llama.com/llama3_3/license/
  modelVariants:
  - variantId: Llama 3.3 70B Instruct
    modelCard: {
    "accessType": "NOT_LISTED",
    "application": "Other",
    "bias": "",
    "canGuestDownload": false,
    "createdDate": "2025-01-08T04:54:23.525Z",
    "description": "# **Llama-3.3-70B-Instruct Overview**\n\n## **Description:**\n\n**Llama-3.3-70B-Instruct** is an auto-regressive language model that uses an optimized transformer architecture. It is designed for text-based tasks such as multilingual chat, coding assistance, and synthetic data generation, and is particularly optimized for dialogue-based use cases. With 70 billion parameters, it provides strong performance that is comparable to larger models but with lower hardware requirements, and it does not process images or audio.\n\nThis model is ready for commercial/non-commercial use.\n\nThis version introduces support for GB200 NVL72, GH200 NVL2, B200 and NVFP4. CUDA updated to version 12.9. For detailed information, refer to Release [Notes for NVIDIA NIM for LLMs LLM 1.12](https://docs.nvidia.com/nim/large-language-models/latest/release-notes.html). \n\n## **Third-Party Community Consideration**\n\nThis model is not owned or developed by NVIDIA. This model has been developed and built to a third-party's requirements for this application and use case; see link to Non-NVIDIA\\[meta-llama/Llama-3.3-70B-Instruct\\]  \n([https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)). \n\n## **License/Terms of Use:**\n\n**GOVERNING TERMS:** The NIM container is governed by the [NVIDIA Software License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-software-license-agreement/) and the [Product-Specific Terms for NVIDIA AI Products](https://www.nvidia.com/en-us/agreements/enterprise-software/product-specific-terms-for-ai-products/); and the model is governed by the [NVIDIA AI Foundation Models Community License Agreement](https://www.nvidia.com/en-us/agreements/enterprise-software/nvidia-ai-foundation-models-community-license-agreement/). \n\n\n**ADDITIONAL INFORMATION**: [Llama 3.3 Community License Agreement](https://www.llama.com/llama3_3/license/). Built with Llama.\n\n## **Get Help**\n\n### Enterprise Support\nGet access to knowledge base articles and support cases or [submit a ticket](https://www.nvidia.com/en-us/data-center/products/ai-enterprise-suite/support/).\n\nYou are responsible for ensuring that your use of NVIDIA provided models complies with all applicable laws.\n\n## **Deployment Geography:**\n\nGlobal \n\n## **Use Case:**\n\nThis model is intended for developers, researchers, and enterprises. They would integrate it into applications and workflows for a variety of advanced text-based tasks.\n\n* For Conversational AI: Building sophisticated and natural-sounding chatbots for customer service, multilingual virtual assistants, and interactive dialogue systems.  \n* For Software Development: Engineers might use the model as a powerful coding assistant for generating code, debugging, explaining complex algorithms, and writing documentation.  \n* For Content Creation and Analysis: Businesses and content creators might use the model  to draft emails, generate marketing copy, summarize long documents, and create synthetic text data to train other machine learning models.\n\n## **Release Date:**\n\nBuild.Nvidia.com 12/17/2024 via  \n[llama-3.3-70b-instruct Model by Meta | NVIDIA NIM](https://build.nvidia.com/meta/llama-3_3-70b-instruct)\n\nGithub 12/13/2024 via   \n[https://github.blog/changelog/2024-12-13-llama-3-3-70b-instruct-is-now-available-on-github-models-ga/](https://github.blog/changelog/2024-12-13-llama-3-3-70b-instruct-is-now-available-on-github-models-ga/)\n\nHuggingface 12/06/2024 via   \n[https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) \n\n**Reference(s):** \n\n[https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)\n\n## **Model Architecture:** \n\nArchitecture Type: Transformer  \nNetwork Architecture: Llama-3.3-70B\n\nThis model was developed based on Meta-Llama-3.3-70B  \n[https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct).\n\nNumber of model parameters: 7.06*10^10\n\n## **Input:**\n\nInput Type(s): Text \n\nInput Format(s): String \n\nInput Parameters: One-Dimensional (1D)\n\nOther Properties Related to Input: The model processes input as tokens. The maximum context length is 8,192 tokens. Input text strings must be pre-processed by the model's specific Tiktoken tokenizer before being fed into the model.\n\n## **Output:**\n\nOutput Type(s): Text \n\nOutput Format(s): String\n\nOutput Parameters: One-Dimensional (1D)\n\nOther Properties Related to Output: The model generates text as a sequence of tokens. The length of the generated output can be controlled by inference parameters. The raw token output requires post-processing (de-tokenization) to be converted into a human-readable string.\n\nOur AI models are designed and/or optimized to run on NVIDIA GPU-accelerated systems. By leveraging NVIDIA's hardware (e.g. GPU cores) and software frameworks (e.g., CUDA libraries), the model achieves faster training and inference times compared to CPU-only solutions.\n\n## **Software Integration:**\n\nRuntime Engine: vLLM, TensorRT\n\nSupported Hardware Microarchitecture Compatibility:\n\nNVIDIA Ampere  \nNVIDIA Blackwell  \nNVIDIA Hopper  \nNVIDIA Lovelace \n\nPreferred Operating System(s):\n\nLinux   \nWindows\n\nThe integration of foundation and fine-tuned models into AI systems requires additional testing using use-case-specific data to ensure safe and effective deployment. Following the V-model methodology, iterative testing and validation at both unit and system levels are essential to mitigate risks, meet technical and functional requirements, and ensure compliance with safety and ethical standards before deployment.\n\n## **Model Version(s):**\n\nLlama-3.3-70B-Instruct\n\n## **Usage**\n\n### **Use with transformers**\n\nStarting with transformers \\>= 4.45.0 onward, you can run conversational inference using the Transformers pipeline abstraction or by leveraging the Auto classes with the generate() function.\n\nMake sure to update your transformers installation via pip install \\--upgrade transformers.\n\nSee the snippet below for usage with Transformers:\n\n```\nimport transformers\nimport torch\n\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n\noutputs = pipeline(\n    messages,\n    max_new_tokens=256,\n)\nprint(outputs[0][\"generated_text\"][-1])\n```\n\n### **Tool use with transformers**\n\nLLaMA-3.3 supports multiple tool use formats. You can see a full guide to prompt formatting [here](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/).\n\nTool use is also supported through [chat templates](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling) in Transformers. Here is a quick example showing a single simple tool:\n\n```\n# First, define a tool\ndef get_current_temperature(location: str) -> float:\n    \"\"\"\n    Get the current temperature at a location.\n    \n    Args:\n        location: The location to get the temperature for, in the format \"City, Country\"\n    Returns:\n        The current temperature at the specified location in the specified units, as a float.\n    \"\"\"\n    return 22.  # A real function should probably actually get the temperature!\n\n# Next, create a chat and apply the chat template\nmessages = [\n  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n]\n\ninputs = tokenizer.apply_chat_template(messages, tools=[get_current_temperature], add_generation_prompt=True)\n```\n\nYou can then generate text from this input as normal. If the model generates a tool call, you should add it to the chat like so:\n\n```\ntool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\"}}\nmessages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n```\n\nand then call the tool and append the result, with the tool role, like so:\n\n```\nmessages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n```\n\nAfter that, you can generate() again to let the model use the tool result in the chat. Note that this was a very brief introduction to tool calling \\- for more information, see the [LLaMA prompt format docs](https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/) and the Transformers [tool use documentation](https://huggingface.co/docs/transformers/main/chat_templating#advanced-tool-use--function-calling).\n\n### **Use with bitsandbytes**\n\nThe model checkpoints can be used in 8-bit and 4-bit for further memory optimisations using bitsandbytes and transformers\n\nSee the snippet below for usage:\n\n```\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"meta-llama/Llama-3.3-70B-Instruct\"\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n\nquantized_model = AutoModelForCausalLM.from_pretrained(\n    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ninput_text = \"What are we having for dinner?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutput = quantized_model.generate(**input_ids, max_new_tokens=10)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nTo load in 4-bit simply pass load\\_in\\_4bit=True\n\n### **Use with llama**\n\nPlease, follow the instructions in the [repository](https://github.com/meta-llama/llama).\n\nTo download Original checkpoints, see the example command below leveraging huggingface-cli:\n\n```\nhuggingface-cli download meta-llama/Llama-3.3-70B-Instruct --include \"original/*\" --local-dir Llama-3.3-70B-Instruct\n```\n\n## **Training, Testing, and Evaluation Datasets:**\n\n### **Training Dataset**\n\n**Data Modality:** Text \n\n\n**Link:** Undisclosed\n\n**Data Collection Method:** Hybrid: Human, Synthetic, Automated\n\n**Labeling Method:** Hybrid: Human, Synthetic\n\n**Properties:** \n\nThe pre-training dataset contains over 15 trillion (15T) tokens from a diverse mix of publicly available online sources. The fine-tuning dataset consists of prompts and preference-ranked responses designed to improve helpfulness and safety.\n\n### **Testing Dataset**\n\n**Link:** Undisclosed\n\n**Data Collection Method:** Hybrid: Human, Synthetic, Automated\n\n**Labeling Method:** Hybrid: Human, Automated\n\n**Properties:** \n\nThe public datasets cover a wide range of tasks including massive multitask language understanding (MMLU), problem-solving (GSM8K), and code generation (HumanEval). Meta's internal evaluation set contains over 2,000 prompts designed to test for safety and helpfulness across various potentially risky categories. \n\n### **Evaluation Dataset**\n\n**Link:** Undisclosed\n\n**Data Collection Method:** Hybrid: Automated, Human, Synthetic\n\n**Labeling Method:** Hybrid: Human, Automated\n\n**Properties:** \n\nThe public datasets are industry-standard benchmarks designed to evaluate diverse capabilities like general knowledge, reasoning, coding, and math. For example, MMLU tests multitask knowledge, HumanEval tests code generation, and GSM8K tests grade-school math word problems. Meta's private evaluation set contains over 2,000 prompts for assessing safety and helpfulness.\n\n**Detailed Performance:**\n\n| Category | Benchmark | \\# Shots | Metric | Llama 3.1 8B Instruct | Llama 3.1 70B Instruct | Llama-3.3 70B Instruct | Llama 3.1 405B Instruct |\n| ----- | ----- | ----- | ----- | ----- | ----- | ----- | ----- |\n|  | MMLU (CoT) | 0 | macro\\_avg/acc | 73.0 | 86.0 | 86.0 | 88.6 |\n|  | MMLU Pro (CoT) | 5 | macro\\_avg/acc | 48.3 | 66.4 | 68.9 | 73.3 |\n| Steerability | IFEval |  |  | 80.4 | 87.5 | 92.1 | 88.6 |\n| Reasoning | GPQA Diamond (CoT) | 0 | acc | 31.8 | 48.0 | 50.5 | 49.0 |\n| Code | HumanEval | 0 | pass@1 | 72.6 | 80.5 | 88.4 | 89.0 |\n|  | MBPP EvalPlus (base) | 0 | pass@1 | 72.8 | 86.0 | 87.6 | 88.6 |\n| Math | MATH (CoT) | 0 | sympy\\_intersection\\_score | 51.9 | 68.0 | 77.0 | 73.8 |\n| Tool Use | BFCL v2 | 0 | overall\\_ast\\_summary/macro\\_avg/valid | 65.4 | 77.5 | 77.3 | 81.1 |\n| Multilingual | MGSM | 0 | em | 68.9 | 86.9 | 91.1 | 91.6 |\n\n## **Technical Limitations** \n\nTesting conducted to date has not covered, nor could it cover, all scenarios. For these reasons, as with all LLMs, the model's potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying this model in any applications, developers should perform safety testing and tuning tailored to their specific applications. Please refer to available resources including the [Responsible Use Guide](https://llama.meta.com/responsible-use-guide), [Trust and Safety](https://llama.meta.com/trust-and-safety/) solutions, and other [resources](https://llama.meta.com/docs/get-started/) to learn more about responsible development. \n\n## **Inference:**\n\n**Acceleration Engine:** vLLM, TensorRT \n\n**Test Hardware:** \n   \n* B200 SXM   \n* H200 SXM  \n* H100 SXM  \n* A100 SXM 80GB  \n* A100 SXM 40GB  \n* L40S PCIe  \n* A10G  \n* H100 NVL  \n* H200 NVL  \n* GH200 96GB    \n* GB200 NVL72\n* GH200 NVL2\n* RTX 5090  \n* RTX 4090  \n* RTX 6000 Ada\n\n## **Ethical Considerations:**\n\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications. When downloaded or used in accordance with our terms of service, developers should work with their internal model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\nYou are responsible for ensuring that your use of NVIDIA provided models complies with all applicable laws.",
    "displayName": "Llama-3.3-70B-Instruct",
    "explainability": "",
    "framework": "Other",
    "hasPlayground": false,
    "hasSignedVersion": true,
    "isPlaygroundEnabled": false,
    "isPublic": false,
    "isReadOnly": true,
    "labels": [
        "NSPECT-L26U-IFIN",
        "nvaie:model:nvaie_supported",
        "nvidia_nim:model:nimmcro_nvidia_nim",
        "productNames:nim-dev",
        "productNames:nv-ai-enterprise"
    ],
    "latestVersionIdStr": "rtx6000-blackwell-svx4-throughput-bf16-jdijd32qrq",
    "latestVersionSizeInBytes": 148279696110,
    "logo": "https://assets.ngc.nvidia.com/products/api-catalog/images/llama-3_3-70b-instruct.jpg",
    "modelFormat": "N/A",
    "name": "llama-3.3-70b-instruct",
    "orgName": "nim",
    "precision": "N/A",
    "privacy": "",
    "productNames": [
        "nim-dev",
        "nv-ai-enterprise"
    ],
    "publicDatasetUsed": {},
    "publisher": "Meta",
    "safetyAndSecurity": "",
    "shortDescription": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out).",
    "teamName": "meta",
    "updatedDate": "2025-10-28T20:37:46.241Z"
}
    source:
      URL: https://catalog.ngc.nvidia.com/orgs/nim/teams/meta/containers/llama-3.3-70b-instruct
    optimizationProfiles:
    - profileId: nim/meta/llama-3.3-70b-instruct:h200x1-throughput-fp8-r-6bjqwx5a
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H200x1 FP8 Throughput
      ngcMetadata:
        02f132ac03fb2ab51b82d88abce83b64feb565c93ad1d54f3b2ab04b7c86b21f:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H200
            gpu_device: 2335:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 70c427b55c83a3c54340d828ce94b546ad566be2ec930f0bd760a00927b4b180
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2335:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 68GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:b200x1-throughput-nvfp4-1bf1rojpxw
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct B200x1 NVFP4 Throughput
      ngcMetadata:
        09fcf7a392fe17c95e87d390742222a4a904b540f79f7b3b3d414bf3a092660b:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 33af5b8bea236de1a255b3108bcbb55e0dad3135b676d809d8ce339956cf67d4
            number_of_gpus: '1'
            pp: '1'
            precision: nvfp4
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: B200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 41GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:a100x2-throughput-bf16-5hyfmddv4a
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct A100x2 BF16 Throughput
      ngcMetadata:
        12c295e09aa3a3bac95522db7c0af51e27d6a4283b0402298c98691fc121a8ae:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: A100
            gpu_device: 20b2:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 517f622a203fd1bdf58c0ba179d9be37fa1917c1f49e0a5aa85c7f5d3b8731b3
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A100
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 20B2:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 135GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:gb200x2-throughput-bf16-neynbhcsra
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct GB200x2 BF16 Throughput
      ngcMetadata:
        12f9ae91afef2d29f5ef4c312f0922ac8ed5aa877c8c49416b0dfaf9dcb902e0:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: fda9c04c123bfcdfae4f8f81847d3aee5eb51698a39dd5905d6581d780b90209
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: GB200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 134GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:b200x4-latency-bf16-h4d-jgziqw
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct B200x4 BF16 Latency
      ngcMetadata:
        135406168c0a2540196ed6f8003e35f8326cda374c6beb6f92b8b6f4883fbf0d:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 76e7be2bffcb7a930d207aa6f616ba863f1884672c64680ec0fca11bfc88304b
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: B200
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 138GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:a10gx8-latency-bf16-of3qbtqvsg
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct A10Gx8 BF16 Latency
      ngcMetadata:
        168f348ad80045c0a730210c796a66ccf83768df25543f8b0567c1e186be9ad6:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: A10G
            gpu_device: 2237:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 00a7b1bb5360bc13540061f29a07344a7aa2feefc46f7f7ff355131ba9d4690d
            number_of_gpus: '8'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '8'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A10G
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 2237:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 150GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H200_NVLx2 BF16 Throughput
      ngcMetadata:
        195071914f36a70a2b4306853667c37e6dd145c4ed787d099a0be9e75d84c58d:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H200_NVL
            gpu_device: 233b:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 651dca39d0943930cc8b7bc0b5cd116294a25601c9f43deab2e362e3c96fde11
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H200_NVL
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 233B:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct A100_SXM4_40GBx8 BF16 Throughput
      ngcMetadata:
        230323019f91e55e7e5ef0f472984bfe38672edc42d5d8f301887842e303e866:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: A100_SXM4_40GB
            gpu_device: 20b0:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 165e61398618addb727e82b8809cea1215b044020a8568597a31d7bee23b05e8
            number_of_gpus: '8'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '8'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A100_SXM4_40GB
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 20B0:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H100_NVLx2 BF16 Throughput
      ngcMetadata:
        252cb13923588a782037650b182dcc87562a58a6a1dc48a31519f9964dee57bd:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100_NVL
            gpu_device: 2321:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: adda34b085d63494164d063e4a82677e59bcde4543da432d4a550a84185434e0
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H100_NVL
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2321:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:b200x2-latency-fp8-n6ww5ulixq
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct B200x2 FP8 Latency
      ngcMetadata:
        2d46c8f638e9000b9892b517219356e3b980aabd33f027e7c858386688febd52:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 00103c1174a5863d30a1429e5aba6b251aa676ec57460280f28c6cb61f117d98
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: B200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 69GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct GH200_144GBx2 BF16 Throughput
      ngcMetadata:
        30809103f16d80f0f834cfec8d3a48617ac311a1e13150534e01d1c34b0a5db7:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GH200_144GB
            gpu_device: 2348:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 70078c4e36e97245d7fe026a7cac6258820c5d8df77bf2d10432c6a35007e7e2
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: GH200_144GB
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2348:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:h200x4-latency-bf16-cp-xxbkpta
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H200x4 BF16 Latency
      ngcMetadata:
        41bc6ff1de6d3dcfe33b8070b32a89946b55b5770c92c82ffb8bb87b8e3fc9d7:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H200
            gpu_device: 2335:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 7c9d81be68e9ba750798e8c48585ebbce4d271d36981e30f09019e011d8e389a
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H200
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2335:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 139GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:a100x8-latency-bf16-qfohcfr1iq
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct A100x8 BF16 Latency
      ngcMetadata:
        443b4edfa5128abcbc85f57ca43e02053730a3fc22929e4b7864422cf5b12d16:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: A100
            gpu_device: 20b2:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: d8669419688b2ce0f64218aeaa11f4840e272e2d1f5d11fc5e0d1b3f53476e2d
            number_of_gpus: '8'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '8'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A100
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 20B2:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 147GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct GH200_144GBx2 FP8 Latency
      ngcMetadata:
        44edc112b59ec6736bc9fc172d7219b9999f4398e5b61a7ca692a2053e1f4fc0:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GH200_144GB
            gpu_device: 2348:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: d6f8ec1ae3a5910ae26ff689e3416a0947cf1c1c1bcc7dfc8d3186e490bcb36c
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GH200_144GB
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2348:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct GH200_144GBx2 FP8 Throughput
      ngcMetadata:
        4a4dc27109678a256cf4ae5209280f044a6562ade8c2e5bca3025a096a41c551:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GH200_144GB
            gpu_device: 2348:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: e075a3b9652dae46b800108afda2a6f7c0f6301a35a1db3671dc8af30f1fd5a2
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GH200_144GB
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2348:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:gb200x1-throughput-nvfp4-ujocyfzf6a
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct GB200x1 NVFP4 Throughput
      ngcMetadata:
        4bf0e1bc784ba2c8b1ae399bb1042d1546bac30df98d02663d4e1db60744aabc:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: cf417dfa6ac83cc20cfb5404ec0b2eae321d174bc4808d6007df8562ffee63d8
            number_of_gpus: '1'
            pp: '1'
            precision: nvfp4
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: GB200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 41GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:l40sx4-latency-fp8-vl02sw2m-g
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct L40Sx4 FP8 Latency
      ngcMetadata:
        52050fe50397b0b158fafe24a0c1e74efad0d04351274757337c86fc99968dd9:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: L40S
            gpu_device: 26b9:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: a9d77447899d9eb9de5254bf262250c7321a6522f30d485abd4072fc1de36dcc
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: L40S
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 26B9:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 69GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct GH200_144GBx1 FP8 Throughput
      ngcMetadata:
        5b6330f563a4c3f73c9b02dc126295dd85d954c0623581981d1c6179155d9f7b:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GH200_144GB
            gpu_device: 2348:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 0f31befe5670c8fd4ae2429ceaa76edcfcbcdfb96db0375e2671df999e4038c7
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GH200_144GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2348:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct GH200_480GBx1 FP8 Latency
      ngcMetadata:
        5e578516ea42fae60c4f314736e8d3e506c497894e059a6af96bd4c2c84edf23:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GH200_480GB
            gpu_device: 2342:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 68e492b05ff7304cf14489a9a313eb7049ab552fb27b106d1dc61af71a5b7c29
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GH200_480GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2342:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:a10gx8-throughput-bf16-rl2yes9ktw
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct A10Gx8 BF16 Throughput
      ngcMetadata:
        613255b124f05cbf875c142c5ea7c2e3ebb7754a8a5473ad828d2bb07e2eaa88:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: A10G
            gpu_device: 2237:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: c0f0a6abdd6734299ec6f65611fa66490fd46303035100f9c865dd5d3c1dfb19
            number_of_gpus: '8'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '8'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A10G
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 2237:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 150GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:h200x2-throughput-bf16-lciwvjwxkw
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H200x2 BF16 Throughput
      ngcMetadata:
        64878d614ca9a859228cd55d140af0865823c2f3524e43c7be53c01c039481b6:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H200
            gpu_device: 2335:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 824477061f9c69bd79fd248a136a273e8d861d092fb853ede5e06e12510d8188
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2335:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 134GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct RTX6000_BLACKWELL_SVx8 BF16 Latency
      ngcMetadata:
        706687e8d19dccfb16a39808c18e54b9e55f7a5d6c2384df2c805453445ee4bb:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 095fa13f893ed4235a19615963e6b18bdb3e599ad5631c493007ae59dfe73f46
            number_of_gpus: '8'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '8'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:a100x4-throughput-bf16-lyvveim8va
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct A100x4 BF16 Throughput
      ngcMetadata:
        76e28450af746bb7626af7e5e2db4b57b56f11f5b6632a120eefabba925c2b15:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: A100
            gpu_device: 20b2:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: f14e1bad1a0e78da150aeedfee7919ab3ef21def09825caffef460b93fdde9b7
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A100
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 20B2:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 140GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:l40sx4-throughput-bf16-rhzeshgk8w
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct L40Sx4 BF16 Throughput
      ngcMetadata:
        7cb838de5dad2c42066f0616756d0ad2708939c450b95416d41098e9931470c1:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: L40S
            gpu_device: 26b9:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: c419c6ba54c118a6deb6ed9918e9c72e7f151698116c1d3c2bc32042a94d6bbb
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: L40S
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 26B9:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 140GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:h100x2-throughput-bf16-m9pz-s1ymq
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H100x2 BF16 Throughput
      ngcMetadata:
        7ed84ed093e8c5e8d237966262d640c6c2f160a8606df22e869e6f7a5a83cc96:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 5006533ca6b5151e94f18d8e518c68965918f248d0680b23e9fc0e4553e0d9ef
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H100
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2330:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 134GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:h100x2-throughput-fp8-wbna-gqhxw
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H100x2 FP8 Throughput
      ngcMetadata:
        7f4107d806d19c2c2beb2e870bf01217de37a247f27ee168985fc42a9576c641:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 0013e870ea929584ec13dad6948450024cdc6c2f03a865f1b050fb08b9f64312
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2330:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 69GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct A100_SXM4_40GBx8 BF16 Latency
      ngcMetadata:
        814d03ce098b7de458602c7bce320c3d06fe898759577c849a34193653a70bbb:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: A100_SXM4_40GB
            gpu_device: 20b0:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 7a207406eaa12a8bb549ea578116338e4e204d3b38ed0ffb6a9d9d789f2cd994
            number_of_gpus: '8'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '8'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: A100_SXM4_40GB
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 20B0:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H200_NVLx1 FP8 Throughput
      ngcMetadata:
        829d3e1c28ffd52afed2d35e9374cfc7b605eda5a630bc9b33fbea5500da8fb3:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H200_NVL
            gpu_device: 233b:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 34a1cf18c4d7501df008280668fe6df7de1f91ff29daee8d5c80291dd6e51b0e
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H200_NVL
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 233B:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:b200x2-latency-nvfp4-prgjwnsudw
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct B200x2 NVFP4 Latency
      ngcMetadata:
        8abcb1c5fc3e57d712a311f08f9b33b59b383196b95f7d7f66c758de85d56567:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 807294ccda05820ac7bbb9cf0471df7494e947226acff080c0782bda0c7d4394
            number_of_gpus: '2'
            pp: '1'
            precision: nvfp4
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: B200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 41GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:h200x2-latency-fp8-ozazyo6fjw
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H200x2 FP8 Latency
      ngcMetadata:
        92e9707e66c742310e9a7a6d38e162b2578375c8fe0844939c499a00116a994e:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H200
            gpu_device: 2335:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 138ef4644a3d6477c3deaf2cd22f548d3396925db62f4752fb73b52b7b8a4a29
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2335:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 69GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:gb200x1-latency-nvfp4-gbqmrrkwrw
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct GB200x1 NVFP4 Latency
      ngcMetadata:
        a1366af9ab8c32f147d10d0fcc2a43d55b20f2c79178b4a291caa5dec55f966c:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 19ca51edfcfaecd4c68b0950ff57be89e59def4ad003dbcfae4352b43d152223
            number_of_gpus: '1'
            pp: '1'
            precision: nvfp4
            profile: latency
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: GB200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 41GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:h100x4-latency-fp8-mg52y2fpwq
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H100x4 FP8 Latency
      ngcMetadata:
        a2003c7b2b19b79aefb52cd9daa58fb20f0520dd9759037ff34e67110f384218:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 8a5f27c50cf45f7d1a1e504bcd33820eefa80539b94a68bbf015c3f4f4cb2c3f
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2330:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 69GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct RTX6000_BLACKWELL_SVx4 BF16 Throughput
      ngcMetadata:
        a403f6513a44565063a70541681355465810849c0f537c825cd6575c960c2c14:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 4896c9f159be7403ca983e4da47959b87841d5fe0034304ab473baf61f3132a1
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:gb200x1-latency-fp8-uepcd7pd4a
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct GB200x1 FP8 Latency
      ngcMetadata:
        a425a0f4eef147092d6d41acbd7c9c3408614205b8135699274b02f2363b707c:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 0cedb0518e3995aa41d37920a83b151ad05bdf2a43beedbff21b709cf696e350
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GB200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 68GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct GH200_144GBx2 BF16 Latency
      ngcMetadata:
        a6f328cf048298b737a05799b74a3f81b4a215f125d71088054c6c32f3446801:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GH200_144GB
            gpu_device: 2348:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 7e9757ebb03d4334fd350490505620d2af6b5329aa8a28df931e0a22e46d55cd
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: GH200_144GB
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2348:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct RTX6000_BLACKWELL_SVx2 FP8 Throughput
      ngcMetadata:
        a9f34dd0f8e4fd295b0d04067aa0ecce24aa3707b26305e9ab084d430546975c:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 77ab630b949b0a58ad580a22ea055bc392a30fbf57357d6398814e00775aab8c
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:gb200x1-throughput-fp8-ybdaheki0g
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct GB200x1 FP8 Throughput
      ngcMetadata:
        af09a13bcaa3650952df251a0dfd03dabaf7700a6d00b6f2264b2c9ef757fbb6:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: b6dc07bb5bf5be874355bbe6288ca066c605a43c23d6c537ac9d4929c22d2cdd
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GB200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 68GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct GH200_480GBx1 FP8 Throughput
      ngcMetadata:
        c7fc979432a42458118ab456c33302cbde984c5d8a0035e9d2c1d07b5f3dc0d9:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GH200_480GB
            gpu_device: 2342:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 1dea2d2f10ec64c74ca127f73b52bf5253dfdc91c5cd5da07cb742e166e8a795
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: GH200_480GB
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2342:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:l40sx4-throughput-fp8-sx6as-ue-a
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct L40Sx4 FP8 Throughput
      ngcMetadata:
        cf120c3ecf2025e6a170cb224802ca6a02cbeec3ad74944a69263b3193a64fa2:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: L40S
            gpu_device: 26b9:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: b118ae4fb04a6bbcf439004b94edd4815d2c965a0c692c2b98a790580c9c3f7b
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: L40S
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 26B9:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 69GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H200_NVLx2 FP8 Latency
      ngcMetadata:
        cf5787bfa25e0f21603c8aa6458d2ae062691d0fa81e684dc219082ba39fb1d9:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H200_NVL
            gpu_device: 233b:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 552ad035a0898be37be03c9d539efbda5a7d2f214b2c5950e14bb694ad8329a9
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H200_NVL
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 233B:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct RTX6000_BLACKWELL_SVx2 NVFP4 Throughput
      ngcMetadata:
        d650534ce98fea4bfc9924d77c91fbd8dca227321c35557e924297ab6b9008cb:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 19aeb73125023f25e273ed14ccc69b935b2ce5131d4d91d1b78f3e8bdc0366b7
            number_of_gpus: '2'
            pp: '1'
            precision: nvfp4
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:gb200x4-latency-bf16-vuvdg5jkzq
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct GB200x4 BF16 Latency
      ngcMetadata:
        d7ff5f88620f7fbe0538931af334663b43d15cb2c969e7fc96375ac60108906f:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: GB200
            gpu_device: 2941:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: be07050242f7ce67689c0d81de40bb1de6967dd251a881bcb784193fb92d8183
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: GB200
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2941:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 138GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:b200x2-throughput-bf16-omzr8lu67g
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct B200x2 BF16 Throughput
      ngcMetadata:
        e0ac049ec460cc8dfe59feaec6d12ae55807dac2b0bd62396c36679f2674e330:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 6d1452af26f860b53df112c90f6b92f22a41156c09dafa2582c2c1194e56a673
            number_of_gpus: '2'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: B200
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 134GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H100_NVLx2 FP8 Throughput
      ngcMetadata:
        e518c22e6d4135300fc5c10bd0c4d195c51ac596e8950172e303bcce84794732:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100_NVL
            gpu_device: 2321:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 3035d73242fb579040fb3f341adc36a7073f780419e73dd97edb7ce35cb0f550
            number_of_gpus: '2'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100_NVL
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2321:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:l40sx4-latency-bf16-rasfmhw4uw
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct L40Sx4 BF16 Latency
      ngcMetadata:
        e6d1855d3f24e439b904cf1fd47d3e136bec4af9134c039558c61f9ae34593af:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: L40S
            gpu_device: 26b9:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 8747b7e093d3b26e808e8bbebdb50c3ac0a0f82402c58b3430a8760ff96e406e
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: L40S
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 26B9:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 138GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct RTX6000_BLACKWELL_SVx4 FP8 Latency
      ngcMetadata:
        ee0b992fafa65ffe00e8df84f80f9e417a400ec40b60b6769db81498482610d7:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 3140e28251686b824ea3fd4d45a86cef01b156d1737ada0b6783b612ac3b6e92
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H100_NVLx4 BF16 Latency
      ngcMetadata:
        efcb2762954af78c9b84774917daf706fd8d663df3d54c298a1fb9d2fb86a119:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100_NVL
            gpu_device: 2321:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 3d2f50e0423aa98250617f6a0dad719bed6892994a47c60e092ce494d93e9bce
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H100_NVL
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2321:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:b200x1-throughput-fp8-xk4doibibg
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct B200x1 FP8 Throughput
      ngcMetadata:
        f215c1f1608a7818f6c465646f8f8cb412a58b39c99e4a15857466fb9a970aef:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: B200
            gpu_device: 2901:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 6979353282e6f8421f9ffd76c33eb1e675f796fc7ed036c6038b99a21d649f18
            number_of_gpus: '1'
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '1'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: FP8
      - key: GPU
        value: B200
      - key: COUNT
        value: 1
      - key: GPU DEVICE
        value: 2901:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 68GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct RTX6000_BLACKWELL_SVx4 NVFP4 Latency
      ngcMetadata:
        f9c5befd972751383a8dfa7b38fb77fd4c69af4e015136f0a194b7db0176ce59:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: RTX6000_BLACKWELL_SV
            gpu_device: 2bb5:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 4f697999cecdc5afc7ff8f588b71a5b7683117aa866f34ab76886db2dbe86dcc
            number_of_gpus: '4'
            pp: '1'
            precision: nvfp4
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: NVFP4
      - key: GPU
        value: RTX6000_BLACKWELL_SV
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2BB5:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:h100x4-throughput-bf16-bpwvcpvnsq
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H100x4 BF16 Throughput
      ngcMetadata:
        fbec99d055ebc70d1261d9520f1f6f854fb0a84771bdadde30668dca1f081c7d:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 2eb1d578e4e069c384bf617e5354889d043a1c72b77f432c07e06ffb1b8be36b
            number_of_gpus: '4'
            pp: '1'
            precision: bf16
            profile: throughput
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: THROUGHPUT
      - key: PRECISION
        value: BF16
      - key: GPU
        value: H100
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2330:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 139GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
    - profileId: nim/meta/llama-3.3-70b-instruct:6f6073b423013f6a7d4d9f39144961bfbfbc386b
      framework: TensorRT-LLM
      displayName: Llama 3.3 70B Instruct H100_NVLx4 FP8 Latency
      ngcMetadata:
        ff1a26a9837e3e3122a70f91d46181b15f22ba8276c47b0d852cabde8a6a5460:
          model: meta/llama-3.3-70b-instruct
          release: 1.14.0
          tags:
            feat_lora: 'false'
            gpu: H100_NVL
            gpu_device: 2321:10de
            llm_engine: tensorrt_llm
            nim_workspace_hash_v1: 9b6105c7bf6521bd8eb6fa1badcd239636f35c06317166bf78d58a8cc239411f
            number_of_gpus: '4'
            pp: '1'
            precision: fp8
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: LATENCY
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100_NVL
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2321:10DE
      - key: NIM VERSION
        value: 1.14.0
      - key: DOWNLOAD SIZE
        value: 263GB
      - key: LLM ENGINE
        value: TENSORRT_LLM
  labels:
  - Llama
  - Meta
  - Chat
  - Text Generation
  - Large Language Model
  - NVIDIA Validated
  config:
    architectures:
    - Other
    modelType: llama
  license: NVIDIA AI Foundation Models Community License
