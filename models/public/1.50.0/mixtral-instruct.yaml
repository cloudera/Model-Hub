models:
- name: Mixtral Instruct
  displayName: Mixtral Instruct
  modelHubID: mixtral-instruct
  category: Text Generation
  type: NGC
  description: The Mixtral Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts model. Mixtral Instruct is a language model that can follow instructions, complete requests, and generate creative text formats. The Mixtral Instruct Large Language Model (LLM) is an instruct fine-tuned version of the Mixtral.
  modelVariants:
  - variantId: Mixtral 8x7B Instruct
    displayName: Mixtral 8x7B Instruct
    source:
      URL: https://catalog.ngc.nvidia.com/orgs/nim/teams/mistralai/containers/mixtral-8x7b-instruct-v01
    optimizationProfiles:
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.12.0+2333135a3-a10gx8-fp16-throughput.1.3.18301798
      displayName: Mixtral 8x7B Instruct A10Gx8 FP16 Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        03501a01c138dcfc63fc672c20053e3fca8d7bdae1f448165d7bed3f241973cf:
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.3.0
          tags:
            feat_lora: false
            gpu: A10G
            gpu_device: 2237:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp16
            profile: throughput
            tp: '8'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: FP16
      - key: GPU
        value: A10G
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 2237:10de
      - key: NIM VERSION
        value: 1.3.0
      - key: DOWNLOAD SIZE
        value: 89GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.12.0+2333135a3-h100x2-int8wo-throughput.1.3.18301798
      displayName: Mixtral 8x7B Instruct H100x2 int8wo Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        208d53be878cb4d31c9019a80637c54e441e4a4edbee17754d1fc1b0b31b1cc1:
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.3.0
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: int8wo
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: INT8WO
      - key: GPU
        value: H100
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.3.0
      - key: DOWNLOAD SIZE
        value: 48GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.12.0+2333135a3-h100x2-fp16-throughput.1.3.18301798
      displayName: Mixtral 8x7B Instruct H100x2 FP16 Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        bbaccf5c5f059943db905cfcb4e9f2e4e83f0da3617abd244b693103d13005f4:
          container_url: nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:1.2.1
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.3.0
          tags:
            feat_lora: false
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp16
            profile: throughput
            tp: '2'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: FP16
      - key: GPU
        value: H100
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.3.0
      - key: DOWNLOAD SIZE
        value: 94GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.12.0+2333135a3-l40sx4-fp8-throughput.1.3.18301798
      displayName: Mixtral 8x7B Instruct L40Sx4 FP8 Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        4a7fcddcd723f52264e0a9b90b3a17674d1ceb11000aa6dfa50e8a9f1d7c4c8e:
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.3.0
          tags:
            feat_lora: 'false'
            gpu: L40S
            gpu_device: 26b5:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: FP8
      - key: GPU
        value: L40S
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 26b5:10de
      - key: NIM VERSION
        value: 1.3.0
      - key: DOWNLOAD SIZE
        value: 94GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.12.0+2333135a3-l40sx4-fp16-throughput.1.3.18301798
      displayName: Mixtral 8x7B Instruct L40Sx4 FP16 Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        536502b5ba23293b7a9bd6dfabd9b93d2d82c8436d0788cc748b28aefd4adf79:
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.3.0
          tags:
            feat_lora: 'false'
            gpu: L40S
            gpu_device: 26b5:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp16
            profile: throughput
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: FP16
      - key: GPU
        value: L40S
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 26b5:10de
      - key: NIM VERSION
        value: 1.3.0
      - key: DOWNLOAD SIZE
        value: 95GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.12.0+2333135a3-h100x4-int8wo-latency.1.3.18301798
      displayName: Mixtral 8x7B Instruct H100 INT8WO Latency
      framework: TensorRT-LLM
      ngcMetadata:
        5cf31967505bc7d4e792563c5521545703cee2be36714b6944e0e33adb70409a:
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.3.0
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: int8wo
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: Latency
      - key: PRECISION
        value: INT8WO
      - key: GPU
        value: H100
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.3.0
      - key: DOWNLOAD SIZE
        value: 48GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.12.0+2333135a3-h100x4-fp16-latency.1.3.18301798
      displayName: Mixtral 8x7B Instruct H100x4 FP16 Latency
      framework: TensorRT-LLM
      ngcMetadata:
        ed45c32307812aa9b45ef8b3f73d635a4ed8af4ee46ffa09253fc529fbfd55db:
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.3.0
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp16
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: Latency
      - key: PRECISION
        value: FP16
      - key: GPU
        value: H100
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.3.0
      - key: DOWNLOAD SIZE
        value: 95GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.12.0+2333135a3-h100x4-fp8-latency.1.3.18301798
      displayName: Mixtral 8x7B Instruct H100x4 FP8 Latency
      framework: TensorRT-LLM
      ngcMetadata:
        f255f2c7d6787f8b436aa1a74280ebb1a736fa21ae39fd56aeef92f10f7c9c81:
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.3.0
          tags:
            feat_lora: 'false'
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp8
            profile: latency
            tp: '4'
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: Latency
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.3.0
      - key: DOWNLOAD SIZE
        value: 48GB
  - variantId: Mixtral 8x22B Instruct
    displayName: Mixtral 8x22B Instruct
    source:
      URL: https://catalog.ngc.nvidia.com/orgs/nim/teams/mistralai/containers/mixtral-8x22b-instruct-v01
    optimizationProfiles:
    - profileId: nim/mistralai/mixtral-8x22b-instruct-v01:0.10.1+79a76176-h100x8-int8wo-throughput.1.2.2.16140417
      displayName: Mixtral 8x22B Instruct H100 int8wo Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        4ad9a208ce0f8ec41cd6b8681cd0ddf6fbeb406efb3d9baf6847a3fb8bac5863:
          container_url: nvcr.io/nim/mistralai/mixtral-8x22b-instruct-v01:1.0.0
          model: mistralai/mixtral-8x22b-instruct-v0.1
          model_type: text_generation
          release: 1.0.0
          tags:
            feat_lora: false
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: int8wo
            profile: throughput
            tp: '8'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x22b-instruct-v01:hf-52572b2
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                - !name 'rank2.engine'
                - !name 'rank3.engine'
                - !name 'rank4.engine'
                - !name 'rank5.engine'
                - !name 'rank6.engine'
                - !name 'rank7.engine'
                - !name 'trt_llm_config.yaml'
                repo_id: ngc://nim/mistralai/mixtral-8x22b-instruct-v01:0.10.1+79a76176-h100x8-int8wo-throughput.1.0.0.16140417
      sha: 4ad9a208ce0f8ec41cd6b8681cd0ddf6fbeb406efb3d9baf6847a3fb8bac5863
      modelFormat: trt-llm
      latestVersionSizeInBytes: 144762798586
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: int8wo
      - key: GPU
        value: H100
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.2.2
      - key: DOWNLOAD SIZE
        value: 144GB
    - profileId: nim/mistralai/mixtral-8x22b-instruct-v01:0.11.1+14957bf8-h100x8-fp16-throughput.1.1.2.17572569
      displayName: Mixtral 8x22B Instruct H100 FP16 Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        e44c755ef6628cccb74ccf58af4a6efa039f7e49e07a9dd7a27eb17f6500964e:
          container_url: nvcr.io/nim/mistralai/mixtral-8x22b-instruct-v01:1.2.2
          model: mistralai/mixtral-8x22b-instruct-v0.1
          release: 1.2.2
          tags:
            feat_lora: false
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp16
            profile: throughput
            tp: '8'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                - !name 'tool_use_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x22b-instruct-v01:hf-1702b01-tool-calling
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                - !name 'rank2.engine'
                - !name 'rank3.engine'
                - !name 'rank4.engine'
                - !name 'rank5.engine'
                - !name 'rank6.engine'
                - !name 'rank7.engine'
                repo_id: ngc://nim/mistralai/mixtral-8x22b-instruct-v01:0.11.1+14957bf8-h100x8-fp16-throughput.1.1.2.17572569
      sha: e44c755ef6628cccb74ccf58af4a6efa039f7e49e07a9dd7a27eb17f6500964e
      modelFormat: trt-llm
      latestVersionSizeInBytes: 285170977174
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: FP16
      - key: GPU
        value: H100
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.2.2
      - key: DOWNLOAD SIZE
        value: 285GB
  labels:
  - Mistral
  - Instruct
  - Large Language Model
  - TensorRT-LLM
  - Language Generation
  - NeMo
  - NVIDIA Validated
  config:
    architectures:
    - Other
    modelType: mistral
  license: NVIDIA AI Foundation Models Community License