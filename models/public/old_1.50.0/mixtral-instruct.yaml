models:
- name: Mixtral Instruct
  displayName: Mixtral Instruct
  modelHubID: mixtral-instruct
  category: Text Generation
  type: NGC
  description: The Mixtral Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts model. Mixtral Instruct is a language model that can follow instructions, complete requests, and generate creative text formats. The Mixtral Instruct Large Language Model (LLM) is an instruct fine-tuned version of the Mixtral.
  modelVariants:
  - variantId: Mixtral 8x7B Instruct
    displayName: Mixtral 8x7B Instruct
    source:
      URL: https://catalog.ngc.nvidia.com/orgs/nim/teams/mistralai/containers/mixtral-8x7b-instruct-v01
    optimizationProfiles:
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-a10gx8-fp16-throughput.1.1.2.17537111
      displayName: Mixtral 8x7B Instruct A10G FP16 Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        03c5e4ff6a27a2df38cee91e3db5d63451429750086bfb861d1223d39869a931:
          container_url: nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:1.2.1
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.2.1
          tags:
            feat_lora: false
            gpu: A10G
            gpu_device: 2237:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp16
            profile: throughput
            tp: '8'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'NOTICE'
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:hf-a60832c-b
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                - !name 'rank2.engine'
                - !name 'rank3.engine'
                - !name 'rank4.engine'
                - !name 'rank5.engine'
                - !name 'rank6.engine'
                - !name 'rank7.engine'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-a10gx8-fp16-throughput.1.1.2.17537111
      sha: 03c5e4ff6a27a2df38cee91e3db5d63451429750086bfb861d1223d39869a931
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: FP16
      - key: GPU
        value: A10G
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 2237:10de
      - key: NIM VERSION
        value: 1.2.1
      - key: DOWNLOAD SIZE
        value: 89GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-h100x2-fp8-throughput.1.1.2.17538847
      displayName: Mixtral 8x7B Instruct H100 FP8 Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        00056b81c2e41eb9b847342ed553ae88614f450f3f15eebfd2ae56174484bacd:
          container_url: nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:1.2.1
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.2.1
          tags:
            feat_lora: false
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp8
            profile: throughput
            tp: '2'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'NOTICE'
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:hf-a60832c-b
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-h100x2-fp8-throughput.1.1.2.17538847
      sha: 00056b81c2e41eb9b847342ed553ae88614f450f3f15eebfd2ae56174484bacd
      modelFormat: trt-llm
      latestVersionSizeInBytes: 47143615504
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.2.1
      - key: DOWNLOAD SIZE
        value: 47GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-h100x2-int8wo-throughput.1.1.2.17538847
      displayName: Mixtral 8x7B Instruct H100 int8wo Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        01f1ad019f55abb76f10f1687f76ea8e5d2f3d51d6831ddc582d979ff210b4cb:
          container_url: nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:1.2.1
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.2.1
          tags:
            feat_lora: false
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: int8wo
            profile: throughput
            tp: '2'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'NOTICE'
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:hf-a60832c-b
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-h100x2-int8wo-throughput.1.1.2.17538847
      sha: 01f1ad019f55abb76f10f1687f76ea8e5d2f3d51d6831ddc582d979ff210b4cb
      modelFormat: trt-llm
      latestVersionSizeInBytes: 47352822039
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: int8wo
      - key: GPU
        value: H100
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.2.1
      - key: DOWNLOAD SIZE
        value: 47GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-h100x2-fp16-throughput.1.1.2.17538847
      displayName: Mixtral 8x7B Instruct H100 FP16 Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        1f859af2be6c57528dc6d32b6062c9852605d8f2d68bbe76a43b65ebc5ac738d:
          container_url: nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:1.2.1
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.2.1
          tags:
            feat_lora: false
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp16
            profile: throughput
            tp: '2'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'NOTICE'
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:hf-a60832c-b
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-h100x2-fp16-throughput.1.1.2.17538847
      sha: 1f859af2be6c57528dc6d32b6062c9852605d8f2d68bbe76a43b65ebc5ac738d
      modelFormat: trt-llm
      latestVersionSizeInBytes: 93808041027
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: FP16
      - key: GPU
        value: H100
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.2.1
      - key: DOWNLOAD SIZE
        value: 94GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-a100x2-fp16-throughput.1.1.2.17538847
      displayName: Mixtral 8x7B Instruct A100 FP16 Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        9865374899b6ac3a1e25e47644f3d66753288e9d949d883b14c3f55b98fb2ebc:
          container_url: nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:1.2.1
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.2.1
          tags:
            feat_lora: false
            gpu: A100
            gpu_device: 20b2:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp16
            profile: throughput
            tp: '2'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'NOTICE'
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:hf-a60832c-b
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-a100x2-fp16-throughput.1.1.2.17538847
      sha: 9865374899b6ac3a1e25e47644f3d66753288e9d949d883b14c3f55b98fb2ebc
      modelFormat: trt-llm
      latestVersionSizeInBytes: 93804315823
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: FP16
      - key: GPU
        value: A100
      - key: COUNT
        value: 2
      - key: GPU DEVICE
        value: 20b2:10de
      - key: NIM VERSION
        value: 1.2.1
      - key: DOWNLOAD SIZE
        value: 94GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-h100x4-fp16-latency.1.1.2.17538847
      displayName: Mixtral 8x7B Instruct H100 FP16 Latency
      framework: TensorRT-LLM
      ngcMetadata:
        9972482479f39ecacc3f470aaa7d0de7b982a1b18f907aafdb8517db5643e05a:
          container_url: nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:1.2.1
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.2.1
          tags:
            feat_lora: false
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp16
            profile: latency
            tp: '4'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'NOTICE'
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:hf-a60832c-b
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                - !name 'rank2.engine'
                - !name 'rank3.engine'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-h100x4-fp16-latency.1.1.2.17538847
      sha: 9972482479f39ecacc3f470aaa7d0de7b982a1b18f907aafdb8517db5643e05a
      modelFormat: trt-llm
      latestVersionSizeInBytes: 94475885022
      spec:
      - key: PROFILE
        value: Latency
      - key: PRECISION
        value: FP16
      - key: GPU
        value: H100
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.2.1
      - key: DOWNLOAD SIZE
        value: 94GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-l40sx4-fp16-throughput.1.1.2.17538847
      displayName: Mixtral 8x7B Instruct L40S FP16 Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        ad3c46c1c8d71bb481205732787f2c157a9cfc9b6babef5860518a047e155639:
          container_url: nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:1.2.1
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.2.1
          tags:
            feat_lora: false
            gpu: L40S
            gpu_device: 26b5:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp16
            profile: throughput
            tp: '4'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'NOTICE'
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:hf-a60832c-b
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                - !name 'rank2.engine'
                - !name 'rank3.engine'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-l40sx4-fp16-throughput.1.1.2.17538847
      sha: ad3c46c1c8d71bb481205732787f2c157a9cfc9b6babef5860518a047e155639
      modelFormat: trt-llm
      latestVersionSizeInBytes: 94453103580
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: FP16
      - key: GPU
        value: L40S
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 26b5:10de
      - key: NIM VERSION
        value: 1.2.1
      - key: DOWNLOAD SIZE
        value: 94GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-h100x4-fp8-latency.1.1.2.17538847
      displayName: Mixtral 8x7B Instruct H100 FP8 Latency
      framework: TensorRT-LLM
      ngcMetadata:
        d37580fa5deabc5a4cb17a2337e8cc672b19eaf2791cf319fd16582065e40816:
          container_url: nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:1.2.1
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.2.1
          tags:
            feat_lora: false
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp8
            profile: latency
            tp: '4'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'NOTICE'
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:hf-a60832c-b
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                - !name 'rank2.engine'
                - !name 'rank3.engine'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-h100x4-fp8-latency.1.1.2.17538847
      sha: d37580fa5deabc5a4cb17a2337e8cc672b19eaf2791cf319fd16582065e40816
      modelFormat: trt-llm
      latestVersionSizeInBytes: 47320446534
      spec:
      - key: PROFILE
        value: Latency
      - key: PRECISION
        value: FP8
      - key: GPU
        value: H100
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.2.1
      - key: DOWNLOAD SIZE
        value: 47GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-a100x4-fp16-latency.1.1.2.17538847
      displayName: Mixtral 8x7B Instruct A100 FP16 Latency
      framework: TensorRT-LLM
      ngcMetadata:
        e249e70e3ee390e606782eab19e7a9cf2aeb865bdbc638aaf0fc580901492841:
          container_url: nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:1.2.1
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.2.1
          tags:
            feat_lora: false
            gpu: A100
            gpu_device: 20b2:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp16
            profile: latency
            tp: '4'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'NOTICE'
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:hf-a60832c-b
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                - !name 'rank2.engine'
                - !name 'rank3.engine'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-a100x4-fp16-latency.1.1.2.17538847
      sha: e249e70e3ee390e606782eab19e7a9cf2aeb865bdbc638aaf0fc580901492841
      modelFormat: trt-llm
      latestVersionSizeInBytes: 9094013963
      spec:
      - key: PROFILE
        value: Latency
      - key: PRECISION
        value: FP16
      - key: GPU
        value: A100
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 20b2:10de
      - key: NIM VERSION
        value: 1.2.1
      - key: DOWNLOAD SIZE
        value: 9GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-h100x4-int8wo-latency.1.1.2.17538847
      displayName: Mixtral 8x7B Instruct H100 int8wo Latency
      framework: TensorRT-LLM
      ngcMetadata:
        ee616a54bea8e869009748eefb0d905b168d2095d0cdf66d40f3a5612194d170:
          container_url: nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:1.2.1
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.2.1
          tags:
            feat_lora: false
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: int8wo
            profile: latency
            tp: '4'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'NOTICE'
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:hf-a60832c-b
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                - !name 'rank2.engine'
                - !name 'rank3.engine'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-h100x4-int8wo-latency.1.1.2.17538847
      sha: ee616a54bea8e869009748eefb0d905b168d2095d0cdf66d40f3a5612194d170
      modelFormat: trt-llm
      latestVersionSizeInBytes: 47992100010
      spec:
      - key: PROFILE
        value: Latency
      - key: PRECISION
        value: int8wo
      - key: GPU
        value: H100
      - key: COUNT
        value: 4
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.2.1
      - key: DOWNLOAD SIZE
        value: 47GB
    - profileId: nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-a10gx8-fp16-throughput.1.1.2.17537111
      displayName: Mixtral 8x7B Instruct A10G FP16 Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        03c5e4ff6a27a2df38cee91e3db5d63451429750086bfb861d1223d39869a931:
          container_url: nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:1.2.1
          model: mistralai/mixtral-8x7b-instruct-v0.1
          release: 1.2.1
          tags:
            feat_lora: false
            gpu: A10G
            gpu_device: 2237:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp16
            profile: throughput
            tp: '8'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'NOTICE'
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:hf-a60832c-b
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                - !name 'rank2.engine'
                - !name 'rank3.engine'
                - !name 'rank4.engine'
                - !name 'rank5.engine'
                - !name 'rank6.engine'
                - !name 'rank7.engine'
                repo_id: ngc://nim/mistralai/mixtral-8x7b-instruct-v01:0.11.1+14957bf8-a10gx8-fp16-throughput.1.1.2.17537111
      sha: 03c5e4ff6a27a2df38cee91e3db5d63451429750086bfb861d1223d39869a931
      modelFormat: trt-llm
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: fp16
      - key: GPU
        value: A10G
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 2237:10de
      - key: NIM VERSION
        value: 1.2.1
      - key: DOWNLOAD SIZE
        value: 89GB
  - variantId: Mixtral 8x22B Instruct
    displayName: Mixtral 8x22B Instruct
    source:
      URL: https://catalog.ngc.nvidia.com/orgs/nim/teams/mistralai/containers/mixtral-8x22b-instruct-v01
    optimizationProfiles:
    - profileId: nim/mistralai/mixtral-8x22b-instruct-v01:0.10.1+79a76176-h100x8-int8wo-throughput.1.2.2.16140417
      displayName: Mixtral 8x22B Instruct H100 int8wo Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        4ad9a208ce0f8ec41cd6b8681cd0ddf6fbeb406efb3d9baf6847a3fb8bac5863:
          container_url: nvcr.io/nim/mistralai/mixtral-8x22b-instruct-v01:1.0.0
          model: mistralai/mixtral-8x22b-instruct-v0.1
          model_type: text_generation
          release: 1.0.0
          tags:
            feat_lora: false
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: int8wo
            profile: throughput
            tp: '8'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x22b-instruct-v01:hf-52572b2
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                - !name 'rank2.engine'
                - !name 'rank3.engine'
                - !name 'rank4.engine'
                - !name 'rank5.engine'
                - !name 'rank6.engine'
                - !name 'rank7.engine'
                - !name 'trt_llm_config.yaml'
                repo_id: ngc://nim/mistralai/mixtral-8x22b-instruct-v01:0.10.1+79a76176-h100x8-int8wo-throughput.1.0.0.16140417
      sha: 4ad9a208ce0f8ec41cd6b8681cd0ddf6fbeb406efb3d9baf6847a3fb8bac5863
      modelFormat: trt-llm
      latestVersionSizeInBytes: 144762798586
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: int8wo
      - key: GPU
        value: H100
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.2.2
      - key: DOWNLOAD SIZE
        value: 144GB
    - profileId: nim/mistralai/mixtral-8x22b-instruct-v01:0.11.1+14957bf8-h100x8-fp16-throughput.1.1.2.17572569
      displayName: Mixtral 8x22B Instruct H100 FP16 Throughput
      framework: TensorRT-LLM
      ngcMetadata:
        e44c755ef6628cccb74ccf58af4a6efa039f7e49e07a9dd7a27eb17f6500964e:
          container_url: nvcr.io/nim/mistralai/mixtral-8x22b-instruct-v01:1.2.2
          model: mistralai/mixtral-8x22b-instruct-v0.1
          release: 1.2.2
          tags:
            feat_lora: false
            gpu: H100
            gpu_device: 2330:10de
            llm_engine: tensorrt_llm
            pp: '1'
            precision: fp16
            profile: throughput
            tp: '8'
          workspace: !workspace
            components:
            - dst: ''
              src:
                files:
                - !name 'README.md'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'generation_config.json'
                - !name 'model.safetensors.index.json'
                - !name 'special_tokens_map.json'
                - !name 'tokenizer.json'
                - !name 'tokenizer.model'
                - !name 'tokenizer_config.json'
                - !name 'tool_use_config.json'
                repo_id: ngc://nim/mistralai/mixtral-8x22b-instruct-v01:hf-1702b01-tool-calling
            - dst: trtllm_engine
              src:
                files:
                - !name 'LICENSE.txt'
                - !name 'NOTICE.txt'
                - !name 'checksums.blake3'
                - !name 'config.json'
                - !name 'metadata.json'
                - !name 'rank0.engine'
                - !name 'rank1.engine'
                - !name 'rank2.engine'
                - !name 'rank3.engine'
                - !name 'rank4.engine'
                - !name 'rank5.engine'
                - !name 'rank6.engine'
                - !name 'rank7.engine'
                repo_id: ngc://nim/mistralai/mixtral-8x22b-instruct-v01:0.11.1+14957bf8-h100x8-fp16-throughput.1.1.2.17572569
      sha: e44c755ef6628cccb74ccf58af4a6efa039f7e49e07a9dd7a27eb17f6500964e
      modelFormat: trt-llm
      latestVersionSizeInBytes: 285170977174
      spec:
      - key: PROFILE
        value: Throughput
      - key: PRECISION
        value: FP16
      - key: GPU
        value: H100
      - key: COUNT
        value: 8
      - key: GPU DEVICE
        value: 2330:10de
      - key: NIM VERSION
        value: 1.2.2
      - key: DOWNLOAD SIZE
        value: 285GB
  labels:
  - Mistral
  - Instruct
  - Large Language Model
  - TensorRT-LLM
  - Language Generation
  - NeMo
  - NVIDIA Validated
  config:
    architectures:
    - Other
    modelType: mistral
  license: NVIDIA AI Foundation Models Community License
