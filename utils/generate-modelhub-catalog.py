####################################################################################################################
# Copyright (c) 2025 Cloudera, Inc. All rights reserved.
# This script is to generate modelhub catalog YAML file using the manifest provided by the cml-serving team
# Prerequisties:
#   1. Python 3.8 and above
#   2. If you are running the script for the first time install ruamel.yaml package using "pip install ruamel.yaml"
#   4. ngc-cli setup (refer: https://org.ngc.nvidia.com/setup/installers/cli)
#   3. cml-serving manifest files in your local machine (https://github.infra.cloudera.com/Sense/cml-serving/tree/main/manifests)
#
# Usage:
#       python ./generate-modelhub-catalog.py \
#         --profiles-yaml <Path for manifest file provided by cml-serving> \
#         --base-model-file ./base_model.yaml \
#         --output-yaml <Path where the modelhub catalog file need to be written> \
#         --ngc-api-key <API key to run ngc cli command> \
#         --platform <public|private, default: public> \
#         [--whitelisted-gpus <Space separated GPU types; case-insensitive>] \
#         [--include-vllm]
#         [--create-manifest-path <base output dir>]
#
# E.g (public, whitelist all major GPUs):
#   python ./generate-modelhub-catalog.py --profiles-yaml ~/Documents/llama-3.1-70b-instruct.yaml \
#     --output-yaml ./llama-3.1-70b-instruct.yaml --ngc-api-key <> \
#     --platform public --whitelisted-gpus A10G L40S H100 H200 A100
#
# E.g (private):
#   python ./generate-modelhub-catalog.py --profiles-yaml ~/Documents/llama-3.1-70b-instruct.yaml \
#     --output-yaml ./llama-3.1-70b-instruct.yaml --ngc-api-key <> --platform private
#
# E.g (private, include VLLM generics as-is):
#   python ./generate-modelhub-catalog.py --profiles-yaml ~/Documents/gpt-oss.yaml \
#     --output-yaml ./gpt-oss.yaml --ngc-api-key <> --platform private --include-vllm
#
# Note: 
#   1. If you are generating modelhub catalog for existing model then pass the YAML file of the model by removing all the profiles details under optimizationProfiles and just keep "optimizationProfiles: []"
#   2. If you are generating modelhub catalog for new model then use default mlx-crud-app/service/resources/utils/base_model.yaml and once the profiles are loaded update the catalog file with remanining user defined values
#   3. If you are generating modelgub catalog for a model with multiple variants then use default mlx-crud-app/service/resources/utils/base_model.yaml to generate profiles individually and update the original modelhub catalog YAML by copying the profiles generated by the script.
#        3.1 This is because cml-serving provides different manifests for different variants of same model
#   4. Filtering and inclusion rules (public unless stated otherwise):
#       - GPU whitelist is case-insensitive; if not provided, all supported GPUs are considered.
#       - GPU canonicalization for matching uses `gpu_key` (preferred) or `gpu`, strips vendor prefix (e.g., "NVIDIA "), and uppercases; original tags.gpu is preserved.
#       - Regular profiles are ignored when:
#           a) tags.feat_lora == 'true'
#           b) public and llm_engine == 'vllm' and --include-vllm is NOT set
#           c) public and GPU not in --whitelisted-gpus
#           d) public and A100 with TP*PP > --a100-max-count (default: 1; set 4 to allow up to 4)
#           e) public and H100 with gpu_device != 2330:10de; public and H200 with gpu_device != 2335:10de
#           f) no GPU in tags (handled by generic synthesis paths below)
#       - Generic profile synthesis (public):
#           a) TRT-LLM generics (llm_engine: tensorrt_llm, trtllm_buildable: 'true', feat_lora: 'false') synthesize entries for GPUs [A10G, L40S, H100, H200, A100], respecting whitelist and A100 limit.
#           b) VLLM generics synthesize only if --include-vllm is set, respecting whitelist and A100 limit.
#           c) Capacity check: skip synthesized entries when download size (GB) > (TP*PP* per-GPU memory GB) using built-in GPU_SPECS.
#           d) Generated profiles set gpu_device via mapping; H200 supported (2335:10de).
#       - Private platform generics: included as-is without GPU details (VLLM requires --include-vllm).
#       - Combination tracking ensures we only create missing GPU-TP*PP-PRECISION-PROFILE combinations.
#   5. Display names:
#       - Public: "<Model Name> <GPU>x<count> [SM<sm>] [V<v>] [ONNX] <PRECISION> <Profile>"
#       - Private generics (non-ONNX): "<Model Name> Generic NVIDIA GPUx<count> [SM<sm>] [V<v>] <PRECISION> <Profile>"
#       - Private ONNX: "<Model Name> ONNX <PRECISION> <Profile>"
#         (count = TP*PP)
#   6. Spec population rules:
#       - PROFILE only when non-empty
#       - PRECISION only when non-empty
#       - GPU DEVICE only when non-empty
#       - COUNT always included
#       - Adds keys when present in tags: LLM ENGINE, SM, V, BACKEND, MODEL TYPE, TRTLLM BUILDABLE
#   7. Serialization & metadata:
#       - nim_workspace_hash_v1 is emitted as a single-line, unquoted scalar
#       - Long scalars are not wrapped
#       - ngcMetadata: tags.gpu is uppercased at serialization time (source tags are not mutated)
#   8. Manifest copy:
#       - If --create-manifest-path is set, write a copy of --profiles-yaml to <base>/<profileId_prefix>.yaml (prefix is text before ':'), preserving folder structure; falls back to model when applicable
####################################################################################################################

import argparse
from ruamel.yaml import YAML
from ruamel.yaml.scalarstring import PlainScalarString
import subprocess
import json
import os
import math
import shutil

# GPU memory specs for capacity checks (per-GPU memory in GB)
GPU_SPECS = {
    "L40S": {"memory_gb": 48, "device_id": "26b9:10de"},
    "A10G": {"memory_gb": 24, "device_id": "2237:10de"},
    "H100": {"memory_gb": 80, "device_id": "2330:10de"},
    "H200": {"memory_gb": 141, "device_id": "2335:10de"},
    "A100": {"memory_gb": 80, "device_id": "20b2:10de"},
}

def _parse_gb(size_str: str):
    try:
        s = str(size_str).strip().upper()
        if s.endswith("GB"):
            return int(s[:-2])
        return int(s)
    except Exception:
        return None

def generate_display_name(model, tags):
    model_name = model.split('/')[-1]
    parts = model_name.replace('_', ' ').split('-')
    formatted_parts = []
    for part in parts:
        if part.lower().endswith('b') and part[:-1].replace('.', '').isdigit():
            formatted_parts.append(part[:-1] + 'B')
        else:
            formatted_parts.append(part.capitalize())
    base_name = " ".join(formatted_parts)
    gpu = tags.get("gpu", "").upper()
    tp = int(tags.get("tp", "1"))
    pp = int(tags.get("pp", "1"))
    precision = tags.get("precision", "").upper()
    profile = tags.get("profile", "").capitalize()
    count = tp * pp
    gpu_count = f"{gpu}x{count}" if gpu else ""
    sm_val = str(tags.get("sm", "")).strip()
    v_val = str(tags.get("v", "")).strip()
    sm_part = f"SM{sm_val}" if sm_val else ""
    v_part = f"V{v_val}" if v_val else ""
    onnx_part = "ONNX" if str(tags.get("model_type", "")).lower() == "onnx" else ""
    suffix = " ".join(part for part in [gpu_count, sm_part, v_part, onnx_part, precision, profile] if part)
    return f"{base_name} {suffix}".strip()

def generate_display_name_private(model, tags):
    model_name = model.split('/')[-1]
    parts = model_name.replace('_', ' ').split('-')
    formatted_parts = []
    for part in parts:
        if part.lower().endswith('b') and part[:-1].replace('.', '').isdigit():
            formatted_parts.append(part[:-1] + 'B')
        else:
            formatted_parts.append(part.capitalize())
    base_name = " ".join(formatted_parts)
    tp = int(tags.get("tp", "1"))
    pp = int(tags.get("pp", "1"))
    precision = tags.get("precision", "").upper()
    profile = tags.get("profile", "").capitalize()
    count = tp * pp
    sm_val = str(tags.get("sm", "")).strip()
    v_val = str(tags.get("v", "")).strip()
    sm_part = f"SM{sm_val}" if sm_val else ""
    v_part = f"V{v_val}" if v_val else ""
    onnx_part = "ONNX" if str(tags.get("model_type", "")).lower() == "onnx" else ""
    # For ONNX on private, omit Generic GPUx<count>
    if onnx_part:
        suffix = " ".join(part for part in [onnx_part, precision, profile] if part)
    else:
        # Always include count even without GPU to disambiguate private generics
        suffix = " ".join(part for part in [f"Generic NVIDIA GPUx{count}", sm_part, v_part, precision, profile] if part)
    return f"{base_name} {suffix}".strip()

def profile_id_from_workspace(profile, gpu):
    files = profile.get("workspace", {}).get("files", {})

    def extract_clean_uri(uri):
        if uri.startswith("ngc://"):
            uri = uri[len("ngc://"):]
        return uri.split('?')[0]

    # Prefer URI containing the GPU tag (if available)
    for fdata in files.values():
        uri = fdata.get("uri", "")
        if gpu and gpu.lower() in uri.lower():
            return extract_clean_uri(uri)

    # Fallback: return the first valid URI
    for fdata in files.values():
        uri = fdata.get("uri", "")
        if uri:
            return extract_clean_uri(uri)

    return None

def _get_gpu_tag(tags):
    """Return canonical uppercase GPU from tags['gpu_key'] (preferred) or tags['gpu'].
    Strips common vendor prefixes like 'NVIDIA '. Returns empty string if none.
    """
    raw = str(tags.get("gpu_key", "") or tags.get("gpu", "")).strip()
    up = raw.upper()
    if up.startswith("NVIDIA "):
        up = up[len("NVIDIA "):]
    return up

def should_ignore_profile(tags, whitelisted_gpus, platform="public", include_vllm=False, a100_max_count=1):
    gpu = _get_gpu_tag(tags)
    tp = int(tags.get("tp", "1"))
    pp = int(tags.get("pp", "1"))

    if not gpu:
        return True
    if tags.get("feat_lora", "").lower() == "true":
        return True
    if tags.get("llm_engine", "").lower() == "vllm" and not include_vllm:
        return True
    # If whitelisted_gpus is specified and GPU is not in the whitelist, ignore it
    # Case-insensitive comparison: both manifest GPU and whitelist entries are converted to uppercase
    if whitelisted_gpus:
        whitelisted_gpus_upper = [g.upper() for g in whitelisted_gpus]
        if gpu.upper() not in whitelisted_gpus_upper:
            return True
    # Only apply A100 filtering for public platform
    if platform == "public" and gpu.upper() == "A100" and tp * pp > a100_max_count:
        return True
    return False

def is_generic_profile(tags):
    """Check if a profile is a generic profile that can be used to generate GPU-specific entries"""
    gpu = tags.get("gpu", "").strip()
    llm_engine = tags.get("llm_engine", "").lower()
    trtllm_buildable = tags.get("trtllm_buildable", "").lower()
    feat_lora = tags.get("feat_lora", "").lower()

    # Generic profile criteria: no GPU, tensorrt_llm engine, buildable, not lora
    return (not gpu and
            llm_engine == "tensorrt_llm" and
            trtllm_buildable == "true" and
            feat_lora == "false")

def create_gpu_specific_profile(base_profile, model, release, target_gpu, api_key):
    """Create a GPU-specific optimization profile from a generic profile"""
    # GPU to device ID mapping
    gpu_device_mapping = {
        "L40S": "26b9:10de",
        "A10G": "2237:10de",
        "H100": "2330:10de",
        "H200": "2335:10de",
        "A100": "20b2:10de"
    }

    tags = base_profile.get("tags", {}).copy()
    tags["gpu"] = target_gpu.upper()
    tags["gpu_device"] = gpu_device_mapping.get(target_gpu, "")
    # Ensure nim_workspace_hash_v1 stays single line
    if "nim_workspace_hash_v1" in tags and isinstance(tags["nim_workspace_hash_v1"], str):
        tags["nim_workspace_hash_v1"] = PlainScalarString(tags["nim_workspace_hash_v1"])

    profile_id_uri = profile_id_from_workspace(base_profile, target_gpu)
    if not profile_id_uri:
        return None

    display_name = generate_display_name(model, tags)
    count = int(tags.get("tp", "1")) * int(tags.get("pp", "1"))
    download_size = get_download_size_gb(str(profile_id_uri), api_key)

    # Build spec list, skipping empty values
    spec_list = []
    profile_value = tags.get("profile", "").upper()
    if profile_value:  # Only add PROFILE if it's not empty
        spec_list.append({"key": "PROFILE", "value": profile_value})

    # Add PRECISION only if non-empty
    precision_value = tags.get("precision", "").upper()
    if precision_value:
        spec_list.append({"key": "PRECISION", "value": precision_value})

    spec_list.extend([
        {"key": "GPU", "value": target_gpu.upper()},
        {"key": "COUNT", "value": count}
    ])

    gpu_device_value = tags.get("gpu_device", "").upper()
    if gpu_device_value:  # Only add GPU DEVICE if it's not empty
        spec_list.append({"key": "GPU DEVICE", "value": gpu_device_value})

    spec_list.extend([
        {"key": "NIM VERSION", "value": release},
        {"key": "DOWNLOAD SIZE", "value": download_size}
    ])

    append_optional_spec_fields(spec_list, tags)

    metadata_tags = dict(tags)
    if "gpu" in metadata_tags and isinstance(metadata_tags["gpu"], str):
        metadata_tags["gpu"] = metadata_tags["gpu"].upper()

    return {
        "profileId": profile_id_uri,
        "framework": "TensorRT-LLM",
        "displayName": display_name,
        "ngcMetadata": {
            base_profile.get("id", ""): {
                "model": model,
                "release": release,
                "tags": metadata_tags
            }
        },
        "modelFormat": "trt-llm",
        "spec": spec_list
    }

def create_gpu_specific_profile_vllm(base_profile, model, release, target_gpu, api_key):
    """Create a GPU-specific optimization profile from a generic VLLM profile"""
    gpu_device_mapping = {
        "L40S": "26b9:10de",
        "A10G": "2237:10de",
        "H100": "2330:10de",
        "H200": "2335:10de",
        "A100": "20b2:10de"
    }

    tags = base_profile.get("tags", {}).copy()
    tags["gpu"] = target_gpu.upper()
    tags["gpu_device"] = gpu_device_mapping.get(target_gpu, "")
    # Ensure nim_workspace_hash_v1 stays single line
    if "nim_workspace_hash_v1" in tags and isinstance(tags["nim_workspace_hash_v1"], str):
        tags["nim_workspace_hash_v1"] = PlainScalarString(tags["nim_workspace_hash_v1"])

    profile_id_uri = profile_id_from_workspace(base_profile, target_gpu)
    if not profile_id_uri:
        return None

    display_name = generate_display_name(model, tags)
    count = int(tags.get("tp", "1")) * int(tags.get("pp", "1"))
    download_size = get_download_size_gb(str(profile_id_uri), api_key)

    spec_list = []
    profile_value = tags.get("profile", "").upper()
    if profile_value:
        spec_list.append({"key": "PROFILE", "value": profile_value})

    # Add PRECISION only if non-empty
    precision_value = tags.get("precision", "").upper()
    if precision_value:
        spec_list.append({"key": "PRECISION", "value": precision_value})

    spec_list.extend([
        {"key": "GPU", "value": target_gpu.upper()},
        {"key": "COUNT", "value": count}
    ])

    gpu_device_value = tags.get("gpu_device", "").upper()
    if gpu_device_value:
        spec_list.append({"key": "GPU DEVICE", "value": gpu_device_value})

    spec_list.extend([
        {"key": "NIM VERSION", "value": release},
        {"key": "DOWNLOAD SIZE", "value": download_size}
    ])

    append_optional_spec_fields(spec_list, tags)

    metadata_tags = dict(tags)
    if "gpu" in metadata_tags and isinstance(metadata_tags["gpu"], str):
        metadata_tags["gpu"] = metadata_tags["gpu"].upper()

    return {
        "profileId": profile_id_uri,
        "framework": "VLLM",
        "displayName": display_name,
        "ngcMetadata": {
            base_profile.get("id", ""): {
                "model": model,
                "release": release,
                "tags": metadata_tags
            }
        },
        "modelFormat": "vllm",
        "spec": spec_list
    }

def get_download_size_gb(profile_id, api_key):
    try:
        env = os.environ.copy()
        env["NGC_CLI_API_KEY"] = api_key

        result = subprocess.run(
            ["ngc", "registry", "model", "info", profile_id, "--format_type", "json"],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=True,
            text=True,
            env=env
        )

        info = json.loads(result.stdout)
        size_bytes = info.get("totalSizeInBytes", 0)
        size_gb = math.ceil(size_bytes / (1024 ** 3)) 
        return f"{size_gb}GB"
    except subprocess.CalledProcessError as e:
        print(f"Failed to fetch size for {profile_id}: {e.stderr}")
        return "UNKNOWN"
    except Exception as e:
        print(f"Error processing size for {profile_id}: {e}")
        return "UNKNOWN"

def append_optional_spec_fields(spec_list, tags):
    mapping = {
        "llm_engine": "LLM ENGINE",
        "sm": "SM",
        "v": "V",
        "backend": "BACKEND",
        "model_type": "MODEL TYPE",
        "trtllm_buildable": "TRTLLM BUILDABLE",
    }
    for tag_key, spec_key in mapping.items():
        value = tags.get(tag_key, "")
        if isinstance(value, str):
            raw = value.strip()
        else:
            raw = value
        if raw in (None, ""):
            continue
        # No special unquoting; just uppercase string representation
        spec_list.append({"key": spec_key, "value": str(raw).upper()})

def main():
    parser = argparse.ArgumentParser(description="Generate modelhub catalog YAML file using manifest YAML and base model YAML")
    parser.add_argument(
    "--base-model-file",
    default="./base_model.yaml",
    help="Base model YAML file path (default: ./base_model.yaml)"
)
    parser.add_argument("--profiles-yaml", required=True, help="Path for manifest file provided by cml-serving")
    parser.add_argument("--output-yaml", required=True, help="Path for YAML output")
    parser.add_argument("--whitelisted-gpus", nargs="*", default=[], help="List of GPU tags to include (case-insensitive)")
    parser.add_argument("--ngc-api-key", required=True, help="NGC CLI API Key")
    parser.add_argument("--platform", choices=["public", "private"], default="public", help="Platform type: 'public' (default) or 'private'. Affects filtering and generic profile handling.")
    parser.add_argument("--include-vllm", action="store_true", help="Include VLLM profiles. Public: only GPU-defined VLLM profiles are included. Private: also includes generic (no-GPU) VLLM profiles as-is.")
    parser.add_argument("--a100-max-count", type=int, default=1, help="Public only: Exclude A100 profiles where TP*PP > this value (default: 1). Use 4 to allow up to 4 GPUs.")
    parser.add_argument("--create-manifest-path", help="Base directory to write a copy of --profiles-yaml using path derived from first profileId (prefix before ':') with .yaml extension, preserving folder structure under this base directory.")
    parser.add_argument("--include-onnx", action="store_true", help="Include ONNX profiles as-is when tags.model_type: onnx (framework/modelFormat = ONNX/onnx).")
    args = parser.parse_args()

    yaml = YAML()
    yaml.preserve_quotes = True
    yaml.width = 100000  # avoid line wrapping of long scalars

    with open(args.base_model_file, 'r') as f:
        model_data = yaml.load(f)

    with open(args.profiles_yaml, 'r') as f:
        profile_data = yaml.load(f)

    model = profile_data.get("model", "")
    release = profile_data.get("release", "")
    profiles = profile_data.get("profiles", [])

    optimization_profiles = []
    covered_gpu_combinations = set()  # Track which GPU-TP*PP-PRECISION-PROFILE combinations already have profiles
    target_gpus = ["A10G", "L40S", "H100", "H200", "A100"]  # GPUs to generate from generic profiles

    # First pass: Process regular profiles (those with specific GPUs)
    for profile in profiles:
        tags = profile.get("tags", {})

        # Include ONNX profiles as-is when requested (regardless of GPU presence)
        if args.include_onnx and str(tags.get("model_type", "")).lower() == "onnx":
            # Ensure nim_workspace_hash_v1 stays single line
            if "nim_workspace_hash_v1" in tags and isinstance(tags["nim_workspace_hash_v1"], str):
                tags["nim_workspace_hash_v1"] = PlainScalarString(tags["nim_workspace_hash_v1"]) 
            gpu_for_uri = tags.get("gpu", "") or ""
            profile_id_uri = profile_id_from_workspace(profile, gpu_for_uri)
            if profile_id_uri:
                # Do not mutate tags['gpu']; keep original value if present
                # Choose display name generator based on platform and GPU presence
                if args.platform == "private" and not tags.get("gpu"):
                    display_name = generate_display_name_private(model, tags)
                else:
                    display_name = generate_display_name(model, tags)
                tp_val = int(tags.get("tp", "1")); pp_val = int(tags.get("pp", "1"))
                count = tp_val * pp_val
                download_size = get_download_size_gb(str(profile_id_uri), args.ngc_api_key)
                spec_list = []
                profile_value = str(tags.get("profile", "")).upper()
                if profile_value:
                    spec_list.append({"key": "PROFILE", "value": profile_value})
                precision_value = str(tags.get("precision", "")).upper()
                if precision_value:
                    spec_list.append({"key": "PRECISION", "value": precision_value})
                # Optional GPU fields if GPU present
                if tags.get("gpu"):
                    spec_list.extend([
                        {"key": "GPU", "value": str(tags.get("gpu", "")).upper()},
                        {"key": "COUNT", "value": count},
                    ])
                    gpu_device_value = str(tags.get("gpu_device", "")).upper()
                    if gpu_device_value:
                        spec_list.append({"key": "GPU DEVICE", "value": gpu_device_value})
                else:
                    spec_list.extend([
                        {"key": "COUNT", "value": count},
                    ])
                spec_list.extend([
                    {"key": "NIM VERSION", "value": release},
                    {"key": "DOWNLOAD SIZE", "value": download_size}
                ])
                append_optional_spec_fields(spec_list, tags)
                metadata_tags = dict(tags)
                if "gpu" in metadata_tags and isinstance(metadata_tags["gpu"], str):
                    metadata_tags["gpu"] = metadata_tags["gpu"].upper()
                optimization_profiles.append({
                    "profileId": profile_id_uri,
                    "framework": "ONNX",
                    "displayName": display_name,
                    "ngcMetadata": {
                        profile.get("id", ""): {
                            "model": model,
                            "release": release,
                            "tags": metadata_tags
                        }
                    },
                    "modelFormat": "onnx",
                    "spec": spec_list
                })
                # proceed to next profile
                continue

        # Prefer 'gpu', else 'gpu_key'
        gpu_tag = _get_gpu_tag(tags)
        
        # Skip generic profiles in first pass
        if not gpu_tag:
            continue
 
        # Normalize GPU tag to uppercase
        # Do not overwrite existing tags['gpu'] (e.g., keep 'NVIDIA L40S').
        # Backfill only if missing and gpu_key provided.
        if not str(tags.get("gpu", "")).strip() and str(tags.get("gpu_key", "")).strip():
            tags["gpu"] = str(tags.get("gpu_key", "")).strip()

        # Ensure nim_workspace_hash_v1 stays single line
        if "nim_workspace_hash_v1" in tags and isinstance(tags["nim_workspace_hash_v1"], str):
            tags["nim_workspace_hash_v1"] = PlainScalarString(tags["nim_workspace_hash_v1"])

        if should_ignore_profile(tags, args.whitelisted_gpus, args.platform, args.include_vllm, args.a100_max_count):
            continue
        # Public-only: enforce gpu_device correctness for H100 and H200
        if args.platform == "public":
            gpu_device_actual = tags.get("gpu_device", "").strip().lower()
            if gpu_tag == "H100" and gpu_device_actual != "2330:10de":
                continue
            if gpu_tag == "H200" and gpu_device_actual != "2335:10de":
                continue

        profile_id_uri = profile_id_from_workspace(profile, gpu_tag)
        if not profile_id_uri:
            continue

        display_name = generate_display_name(model, tags)
        count = int(tags.get("tp", "1")) * int(tags.get("pp", "1"))
        download_size = get_download_size_gb(str(profile_id_uri), args.ngc_api_key)

        # Build spec list, skipping empty values
        spec_list = []
        profile_value = tags.get("profile", "").upper()
        if profile_value:  # Only add PROFILE if it's not empty
            spec_list.append({"key": "PROFILE", "value": profile_value})

        # Add PRECISION only if non-empty
        precision_value = tags.get("precision", "").upper()
        if precision_value:
            spec_list.append({"key": "PRECISION", "value": precision_value})

        spec_list.extend([
            {"key": "GPU", "value": gpu_tag.upper()},
            {"key": "COUNT", "value": count}
        ])

        gpu_device_value = tags.get("gpu_device", "").upper()
        if gpu_device_value:  # Only add GPU DEVICE if it's not empty
            spec_list.append({"key": "GPU DEVICE", "value": gpu_device_value})

        spec_list.extend([
            {"key": "NIM VERSION", "value": release},
            {"key": "DOWNLOAD SIZE", "value": download_size}
        ])

        append_optional_spec_fields(spec_list, tags)

        metadata_tags = dict(tags)
        if "gpu" in metadata_tags and isinstance(metadata_tags["gpu"], str):
            metadata_tags["gpu"] = metadata_tags["gpu"].upper()

        optimization_profile = {
            "profileId": profile_id_uri,
            "framework": "TensorRT-LLM",
            "displayName": display_name,
            "ngcMetadata": {
                profile.get("id", ""): {
                    "model": model,
                    "release": release,
                    "tags": metadata_tags
                }
            },
            "modelFormat": "trt-llm",
            "spec": spec_list
        }

        optimization_profiles.append(optimization_profile)
        # Track GPU-TP*PP-PRECISION-PROFILE combination as covered
        gpu_combination = f"{gpu_tag.upper()}-TP{tags.get('tp', '1')}-PP{tags.get('pp', '1')}-{tags.get('precision', '').upper()}-{tags.get('profile', '').upper()}"
        covered_gpu_combinations.add(gpu_combination)

    # Second pass: Process generic profiles differently based on platform
    if args.platform == "public":
        # Public platform: Generate GPU-specific profiles from generic ones to fill missing combinations
        for profile in profiles:
            tags = profile.get("tags", {})

            # Handle generic VLLM synthesis when requested
            if args.include_vllm and not tags.get("gpu", "").strip() and tags.get("llm_engine", "").lower() == "vllm":
                if "nim_workspace_hash_v1" in tags and isinstance(tags["nim_workspace_hash_v1"], str):
                    tags["nim_workspace_hash_v1"] = PlainScalarString(tags["nim_workspace_hash_v1"])
                tp = tags.get("tp", "1")
                pp = tags.get("pp", "1")
                precision = tags.get("precision", "")
                profile_type = tags.get("profile", "")
                # choose GPUs based on whitelist if provided
                gpu_list = [g for g in (args.whitelisted_gpus or ["A10G","L40S","H100","H200","A100"]) ]
                for target_gpu in gpu_list:
                    comb = f"{target_gpu.upper()}-TP{tp}-PP{pp}-{precision.upper()}-{profile_type.upper()}"
                    if comb in covered_gpu_combinations:
                        continue
                    if target_gpu.upper() == "A100" and int(tp) * int(pp) > args.a100_max_count:
                        continue
                    # Capacity check: skip if download size exceeds total GPU memory for this combo
                    profile_id_uri_tmp = profile_id_from_workspace(profile, target_gpu)
                    if profile_id_uri_tmp:
                        dl_str = get_download_size_gb(str(profile_id_uri_tmp), args.ngc_api_key)
                        dl_gb = _parse_gb(dl_str)
                        per_mem = GPU_SPECS.get(target_gpu.upper(), {}).get("memory_gb")
                        if dl_gb is not None and per_mem is not None:
                            total_mem = int(tp) * int(pp) * per_mem
                            if dl_gb > total_mem:
                                continue
                    vllm_profile = create_gpu_specific_profile_vllm(profile, model, release, target_gpu, args.ngc_api_key)
                    if vllm_profile:
                        optimization_profiles.append(vllm_profile)
                        covered_gpu_combinations.add(comb)
                        print(f"Generated VLLM profile for {target_gpu} (TP={tp}, PP={pp}, Precision={precision.upper()}, Profile={profile_type.upper()}) from generic profile {profile.get('id', 'unknown')}")
                continue

            # Only process TRT-LLM generics for standard synthesis
            if not is_generic_profile(tags):
                continue

            # Generate entries for missing GPU-TP*PP-PRECISION-PROFILE combinations
            tp = tags.get("tp", "1")
            pp = tags.get("pp", "1")
            precision = tags.get("precision", "")
            profile_type = tags.get("profile", "")

            for target_gpu in target_gpus:
                # Check if this specific GPU-TP*PP-PRECISION-PROFILE combination already exists
                gpu_combination = f"{target_gpu.upper()}-TP{tp}-PP{pp}-{precision.upper()}-{profile_type.upper()}"
                if gpu_combination in covered_gpu_combinations:
                    continue

                # Skip if GPU is not in whitelist (case-insensitive comparison)
                if args.whitelisted_gpus:
                    whitelisted_gpus_upper = [g.upper() for g in args.whitelisted_gpus]
                    if target_gpu.upper() not in whitelisted_gpus_upper:
                        continue

                # Skip A100 profiles with more than 1 GPU (apply the same rule as regular profiles)
                if target_gpu.upper() == "A100" and int(tp) * int(pp) > args.a100_max_count:
                    continue

                # Capacity check: skip if download size exceeds total GPU memory for this combo
                profile_id_uri_tmp = profile_id_from_workspace(profile, target_gpu)
                if profile_id_uri_tmp:
                    dl_str = get_download_size_gb(str(profile_id_uri_tmp), args.ngc_api_key)
                    dl_gb = _parse_gb(dl_str)
                    per_mem = GPU_SPECS.get(target_gpu.upper(), {}).get("memory_gb")
                    if dl_gb is not None and per_mem is not None:
                        total_mem = int(tp) * int(pp) * per_mem
                        if dl_gb > total_mem:
                            continue

                gpu_profile = create_gpu_specific_profile(profile, model, release, target_gpu, args.ngc_api_key)
                if gpu_profile:
                    optimization_profiles.append(gpu_profile)
                    covered_gpu_combinations.add(gpu_combination)
                    print(f"Generated profile for {target_gpu} (TP={tp}, PP={pp}, Precision={precision.upper()}, Profile={profile_type.upper()}) from generic profile {profile.get('id', 'unknown')}")

    else:  # private platform
        # Private platform: Include generic profiles as-is without GPU details
        for profile in profiles:
            tags = profile.get("tags", {})

            # Only process generic profiles
            if not is_generic_profile(tags):
                # If include_vllm is set and this is a VLLM generic profile (no GPU), include as-is
                if args.include_vllm and tags.get("llm_engine", "").lower() == "vllm" and not tags.get("gpu", "").strip():
                    profile_id_uri = profile_id_from_workspace(profile, "")  # No specific GPU
                    if profile_id_uri:
                        display_name = generate_display_name_private(model, tags)
                        count = int(tags.get("tp", "1")) * int(tags.get("pp", "1"))
                        download_size = get_download_size_gb(str(profile_id_uri), args.ngc_api_key)

                        spec_list = []
                        profile_value = tags.get("profile", "").upper()
                        if profile_value:
                            spec_list.append({"key": "PROFILE", "value": profile_value})
                        precision_value = tags.get("precision", "").upper()
                        if precision_value:
                            spec_list.append({"key": "PRECISION", "value": precision_value})
                        spec_list.extend([
                            {"key": "COUNT", "value": count},
                            {"key": "NIM VERSION", "value": release},
                            {"key": "DOWNLOAD SIZE", "value": download_size}
                        ])
                        append_optional_spec_fields(spec_list, tags)
                        optimization_profiles.append({
                            "profileId": profile_id_uri,
                            "framework": "VLLM",
                            "displayName": display_name,
                            "ngcMetadata": {profile.get("id", ""): {"model": model, "release": release, "tags": tags}},
                            "modelFormat": "vllm",
                            "spec": spec_list
                        })
                        print(f"Added generic VLLM profile {profile.get('id', 'unknown')} without GPU details for private platform")
                continue

            profile_id_uri = profile_id_from_workspace(profile, "")  # No specific GPU
            if not profile_id_uri:
                continue
            # Ensure nim_workspace_hash_v1 stays single line
            if "nim_workspace_hash_v1" in tags and isinstance(tags["nim_workspace_hash_v1"], str):
                tags["nim_workspace_hash_v1"] = PlainScalarString(tags["nim_workspace_hash_v1"])

            display_name = generate_display_name_private(model, tags)
            count = int(tags.get("tp", "1")) * int(tags.get("pp", "1"))
            download_size = get_download_size_gb(str(profile_id_uri), args.ngc_api_key)

            # Build spec list, skipping empty values
            spec_list = []
            profile_value = tags.get("profile", "").upper()
            if profile_value:  # Only add PROFILE if it's not empty
                spec_list.append({"key": "PROFILE", "value": profile_value})

            # Add PRECISION only if non-empty
            precision_value = tags.get("precision", "").upper()
            if precision_value:
                spec_list.append({"key": "PRECISION", "value": precision_value})

            spec_list.extend([
                {"key": "COUNT", "value": count},
                {"key": "NIM VERSION", "value": release},
                {"key": "DOWNLOAD SIZE", "value": download_size}
            ])

            append_optional_spec_fields(spec_list, tags)

            optimization_profile = {
                "profileId": profile_id_uri,
                "framework": "TensorRT-LLM",
                "displayName": display_name,
                "ngcMetadata": {
                    profile.get("id", ""): {
                        "model": model,
                        "release": release,
                        "tags": tags
                    }
                },
                "modelFormat": "trt-llm",
                "spec": spec_list
            }

            optimization_profiles.append(optimization_profile)
            print(f"Added generic profile {profile.get('id', 'unknown')} without GPU details for private platform")

    if optimization_profiles:
        model_data['models'][0]['modelVariants'][0].setdefault('optimizationProfiles', []).extend(optimization_profiles)

    with open(args.output_yaml, 'w') as f:
        yaml.dump(model_data, f)

    # Since the modelhub profileID (extracted from profile_id_from_workspace() has ":" YAML treats it as YAML field but we expect it to be string. So, adding this perl string operation to make it a string)
    try:
        subprocess.run([
            "perl", "-i", "-0pe",
            r's/profileId:\s*\n\s+([^\n]+)/profileId: \1/g',
            args.output_yaml
        ], check=True)
    except subprocess.CalledProcessError as e:
        print("Error during perl post-processing:", e)
    
    print(f"Modelhub catalog YAML successfully written to: {args.output_yaml}")

    # Optionally create per-model manifest file following profileId prefix under provided base path
    if args.create_manifest_path:
        manifest_relative = None
        # Prefer using the first generated optimization profileId
        try:
            first_profile = None
            # Walk the in-memory list we built earlier
            if 'models' in model_data and model_data['models'] and \
               'modelVariants' in model_data['models'][0] and model_data['models'][0]['modelVariants'] and \
               'optimizationProfiles' in model_data['models'][0]['modelVariants'][0] and model_data['models'][0]['modelVariants'][0]['optimizationProfiles']:
                first_profile = model_data['models'][0]['modelVariants'][0]['optimizationProfiles'][0]
            if first_profile and isinstance(first_profile.get('profileId'), str) and ':' in first_profile['profileId']:
                prefix = first_profile['profileId'].split(':', 1)[0]
                manifest_relative = os.path.normpath(f"{prefix}.yaml")
            else:
                # Fallback: infer from model if it looks like a NIM path
                model_field = profile_data.get('model', '')
                if isinstance(model_field, str) and model_field.startswith('nim/'):
                    manifest_relative = os.path.normpath(f"{model_field}.yaml")
        except Exception as e:
            print(f"Warning: unable to determine manifest target path: {e}")

        if manifest_relative:
            manifest_target = os.path.normpath(os.path.join(args.create_manifest_path, manifest_relative))
            os.makedirs(os.path.dirname(manifest_target), exist_ok=True)
            try:
                shutil.copyfile(args.profiles_yaml, manifest_target)
                print(f"Created manifest copy at: {manifest_target}")
            except Exception as e:
                print(f"Error creating manifest copy at {manifest_target}: {e}")
        else:
            print("Warning: --create-manifest-path requested but could not determine target path from profileId/model.")

if __name__ == "__main__":
    main()
